## DISCUSSION: ‘THE SCIENTIFIC

## MODEL OF CAUSALITY’

## Michael E. Sobel*

1. INTRODUCTION

Heckmanadvocatesanapproachtocausalinferencethatdrawsupon
structural modeling of the outcome(s) of interest (which he calls
scientific), and he contrasts this approach sharply with that arising
out of the statistical literature on experimentation. Drawing exten-
sively on several previous papers—for example, Heckman (1997,
2000, 2001) and Heckman and Navarro-Lozano (2004)—Heckman
goes even further here, arguing that the statistical literature on causal
inference is incomplete because it does not attempt to model the
process by which subjects are selected into treatments (or what statis-
ticians have called the ‘‘treatment assignment mechanism’’) and that
this literature confounds the task of defining parameters with the
tasks of identifying and estimating these parameters. I shall return
to these points later.

But whereas Heckman distinguishes sharply between these
approaches (and hence between certain literatures in economics and
statistics), on balance I find the similarities in the approaches he
discusses much more profound than the dissimilarities. To elaborate,
while there has been and continues to be much philosophical
For financial support, I am grateful to the John D. and Catherine T.

MacArthur Foundation. For helpful remarks, I am grateful to the members of
thecausalinferencestudygroupatColumbiaUniversity.Addressallcorrespon-
dence to Michael Sobel, 421 Fayerweather Hall, Columbia University, New
York,NY,10027(ormes105@columbia.edu).

*ColumbiaUniversity
literatures, there are strong similarities in the way that the word
‘‘cause’’ is used. In particular, a causal relation sustains a counter-
factual conditional, while a noncausal relation need not do so.

Second, causal effects are allowed to be heterogeneous across units,
a point that both statisticians and Heckman have emphasized. Third,
dovetailing with this shared perspective on the nature of the causal
relation,thepotentialoutcomesnotationinventedbyNeyman(1923),
widelyusedsincebystatisticiansworkingonexperimentaldesign(see
for example, the textbooks by Cox [1958] and Kempthorne [1952]), is
now standard notation in both these literatures. This notation,
adopted also by Heckman during the latter 1980s and since in a
number ofhis papers(andhere) nicely capturesthe idea that acausal
relationship sustains a counterfactual conditional statement. To be
sure, this idea can be represented in other ways (Robins and
Greenland 2000), but the potential outcomes notation is very easy
to work with and easy to use without leading oneself astray. The
importance of good notation cannot be emphasized strongly enough.

AsWhitehead([1911],1958:39)pointedout,‘‘Byrelievingthebrainof
all unnecessary work, a good notation sets it free to concentrate on
more advanced problems, and in effect increases the power of the
race.’’ I would also propose that the use of a common notation
encourages investigators to think similarly about a problem.

The incorporation of Neyman’s notation into the modern
literature on causal inference is due to Rubin (1974, 1977, 1978,
1980), who, using this notation, saw the applicability of the work
from the statistical literature on experimental design to observational
studiesandgaveexplicitconsiderationtothekeyroleofthetreatment
assignment mechanism in causal inference, thereby extending this
work to observational studies. To be sure, previous workers in statis-
tics and economics (and elsewhere) understood well in a less formal
waytheproblemsofmakingcausalinferencesinobservationalstudies
where respondents selected themselves into treatment groups, as evi-
denced,forexample,byCochran’sworkonmatchingandHeckman’s
workonsampleselectionbias.ButRubin’sworkwasacriticalbreak-
through. The introduction of a suitable notation allowed in principle
the clarification and formalization of the problem of making causal
inferences with non-experimental data. Further, the use of this
notation allowed the tasks of defining etimands to be separated
enabledRubinnotonlytopinpointthekeyroleofthetreatmentassign-
ment mechanism but to state precise conditions under which this
mechanism was ‘‘ignorable’’ or not. These conditions have been used
by subsequent workers (including Heckman) to evaluate and clarify
existing procedures for causal inference (for example, instrumental
variables) and to develop new methods for estimating causal effects
(forexample,bycreatingmatchedsamplesusingpropensityscores).

And fourth, although Heckman criticizes the ‘‘treatment
effects’’ literature for modeling the effects of causes, as opposed to
modeling the causes of effects, the majority of his paper also focuses
on modeling the effects of an intervention (cause) on an outcome of
interest.

That said, some of the problems involved in making causal
inferences about agents who are not (or cannot in practice be)
subjected by an investigator to one or another treatment of interest
will be somewhat different than those that typically arise when a
treatment is applied or not to a plot of land in an agricultural
experiment. In the latter case, where a randomized experiment can
be conducted, the treatment assignment mechanism is essentially a
(possibly biased) coin toss (or a coin toss within distinguishable
types of plots). Causal inference is typically more straightforward
in this case. However, in observational studies, individuals typically
sort themselves into treatment groups and how they do so may also
be of independent interest. Economists often argue that individuals
make choices by behavingas if they are maximizing expected utility.

When the utility associated with making particular choices is also
related to the outcome under consideration, this may create pro-
blems when the agent uses more information than that available to
theeconomist(ortheeconomistsimplydoesnotknowthetreatment
assignment mechanism). In this case, consideration of the available
information may be inadequate for a ‘‘sufficient’’ description of the
process by which agents allocate themselves to treatment groups.

Here, the assignment mechanism cannot be treated as a coin toss
(net of the available information). Thus, methods based on this
premise are inadequate for this case. In other social and behavioral
sciences, where notions of decision making may take on a different
flavor, the above may or may not be problematic. In any event, the
point is that even when the same notion of causation is under
inference should be expected.

In addition, although assumptions are always present, investi-
gatorsdifferintheextenttowhichtheyarecomfortableusingtheseto
make inferences. Heckman advocates herein what he has elsewhere
(Heckman 2000) called a structural approach to estimating causal
parameters. This model based approach can be very powerful and
can be used as a basis for generating inferences that are sometimes
much stronger and broader in scope than those typically made in the
statistical literature Heckman criticizes. But the structural approach
also typically features stronger assumptions. As Heckman (2000)
documents, frustration with the seemingly arbitrary nature of the
assumptions (for example, exclusion restrictions) used to identify
structuralmodels has ledseveral generations ofeconomists toeschew
structural modeling in favor of other approaches, recently including
experimentation(Heckmanisquitecriticalofthis‘‘naturalexperiment
movement;’’ for a nice treatment of the issues see Rosenzweig and
Wolpin(2000)).Asbefore,evenwhenthesamenotionofcausationis
held, some differences in approaches to causal inference should be
expected.

However,ifonetakestheviewthatstatisticalproceduresshould
be tailored to address questions of interest to various constituencies
(for example, different groups of scientists and policymakers), such
differences should be regarded as both natural and desirable.

Accordingly, I aim primarilytogive a balancedoverview oftheissues.

To make my discussion as useful as possible for Sociological
Methodology readers, I sometimes elaborate on material covered
by Heckman in this or previous papers. Second (and primarily for
the same reason), my remarks are organized around the following
four themes in Heckman’s paper: 1) the nature of the causal
relation, 2) definitions of causal estimands, 3) policy evaluation
and forecasting, and 4) the identification and estimation of causal
effects. Although I do not find simultaneous equation models (and
more generally, structural equation models) in their current form
very useful for causal inference, I do not take up this subject here,
in large measure because a thorough treatment would require
substantially increasing the length of an already long discussion.

For some previous and related discussions of causality in
sult Strotz and Wold (1960), Fisher (1970), and Sobel (1990).

To improve readability, in most instances, I include here the
equations referred to, even if some of these already appear in
Heckman’s article. Whenever possible, I use notation similar or iden-
tical to Heckman’s. In some instances, in order to retain consistency
of style, minor deviations have been necessary.

2. THE CAUSAL RELATION
Heckman argues (page 2, this volume) that ‘‘Science is all about
constructing models of the causes of effects’’ (vs. studying the effects
ofcauses).Healsoarguesthatthenotionofcausalityinvolvesmanip-
ulating one or more variables and comparing the outcomes of these
manipulations. Economists perform these (hypothetical) manipula-
tions using models. And since models are mental constructs,
Heckman concludes that causality resides in the mind. In addition,
as different people think different thoughts and therefore construct
different models, causes are ‘‘relative’’ (to use the language of
Collingwood ([1940], 1972). That is, with equal legitimacy, different
investigators may identify different factors as causes, while ignoring
others. This is part of what Heckman refers to as the provisional
nature of causal knowledge. The implications of this are further
discussed in Sobel (1995), which readers may also wish to consult
for more background on the causal relation.

Modelingthecausesofeffectsiscertainlyanimportantscientific
activity. But it should also be understood that many ‘‘scientific’’ ques-
tionsarenotcausal.Forexample,NASArecentlycrashedaprobefrom
the Deep Impact spacecraft into comet Tempel1 with the objective of
learningmoreaboutthestructureandcompositionof cometary nuclei.

See Bunge (1979) for a discussion of the many kinds of noncausal
questionsthatareofscientificinterest.Further,studyingtheeffectsof
causes is an important scientific activity: figuring out the causes of
global warming would be considerably less important if the effects of
global warming were inconsequential.

Next,thenatureofthecausalrelationhasconsumedtheatten-
tion of philosophers since well before Hume, without resolution.

Regularity theories are one attempt to explicate the nature of
thought of as some set of necessary and/or sufficient antecedents for
the effect. Regularity theories are both general and (typically) deter-
ministic.Inaddition,contemporaryregularitytheoriesusuallyrequire
that a causal statement sustain a counterfactual conditional. The
foregoing ideas may be expressed mathematically using functions, as
in an ‘‘all-causes’’ model:
yðsÞ¼g ðx;uÞ; ð1Þ
s
where y(s) is an outcome of interest depending on state s, g is a
s
function of x, a vector of observables, and u, a vector of unobserva-
bles.Byvaryingthecomponentsofg ,effectsofthearguments(which
s
may be state specific) can be defined under suitable conditions. In
other instances, attention focuses on the results of manipulating the
states s, which might, for example, index a set of treatments that are
to be applied.

The antecedents above are also held to have causal priority
overtheeffect.Explicatingthenatureofthispriorityhasproventobe
adifficulttask.Mostphilosophersholdtotheviewthatthereismore
to causal prioritythanmere temporal order.Thus, somesequencesare
toberegardedascausalwhileothersarenot.Althoughmanipulatinga
causeisonewaytoestablishthepriorityofthecauseovertheeffect,in
many theories of causation, including regularity theories, manipul-
ability is not regarded as essential.

On the other hand, manipulability theories of causation
emphasize the ability of a human agent to manipulate a cause: ‘‘A
causeisaneventorstateofthingswhichitisinourpowertoproduce
or prevent, and by producing or preventing which we can produce or
prevent that whose cause it is said to be’’ (Collingwood [1940], 1972:
p. 296–97). Whereas regularity theories are theories about the causes
of effects, manipulability theories are theories about the effects of
causes.Suchtheoriescorrespondmorecloselywiththewayanexperi-
mentalist thinks of causation. These theories are also readily com-
bined with singular theories of the causal relation.

At first glance, it appears that manipulability theories are
inherently at odds with regularity theories. Thus, it might be argued
that the modern literatures on causal inference in both statistics and
econometrics, because these literatures are typically concerned with
for example, the effects of a policy intervention, as in Heckman’s
paper—are, from the scientific standpoint advocated by Heckman,
misdirected. Importantly, this is not the case, and manipulability
theoriescanbereconciledwithregularitytheoriesbynotingthatamanipu-
latedcauseissimplyonecomponentof(1)withuunknown(somecompo-
nentsofumaybeunknownandotherssimplynotobserved).Toillustrate
thispointusing(1),supposethetreatmentvariablesis0(notreatment)or1
(treatment),x2(cid:2) andu2(cid:2) ¼ (cid:2) [(cid:2) ,with(cid:2) \(cid:2) ¼ ;.Suppose
x u 0u 1u 0u 1u
thatfor(x,u) 2 (cid:2) (cid:2) (cid:2) , g (x, u) ¼ g (x, u) ¼ 0, while for (x, u) 2
x 0u 0 1
(cid:2) (cid:2) (cid:2) , g (x, u) ¼ 0, g (x, u) ¼ 1. In a manipulability theory, the
x 1u 0 1
variable s is singled out for attention. If the investigator knows (s, x,
u) and the functions g, the effect of s varies over x and u in a known
s
(to the investigator) way. In practice, the investigator observes only s
andx,inwhichcasetheeffectofs(whichtheinvestigatormaynotbe
able to identify from data) varies over x in an apparently nondeter-
ministic manner.

The relativity of causation (part of what Heckman calls the
provisional nature of causal knowledge) is also easily illustrated. In
(1) the treatment variable s may be singled out for attention and
manipulated. The other arguments (x and u) remain in the causal
background. A different investigator might identify one or more
components of x (cid:3) (x , x ) as the cause and rewrite (1) as h
1 2 x1
(x , u, s).

2
When the cause can be manipulated, each unit in a research
study can receive any of the various levels of the cause (even
though in practice a given unit is observed only at one level). In
his presentation of what has come to be known in the statistical
community as ‘‘Rubin’s model for causal inference,’’ Holland
(1986), like Heckman, also emphasizes the importance of mani-
pulating the cause, going so far as to coin the (unfortunate)
phrase, ‘‘No causation without manipulation.’’ Whereas Holland
appears to insist on the actual ability of an investigator to
manipulate the cause(s), many others, including Heckman, have
argued that it is the idea of manipulating the cause, even if this can
only be done hypothetically, that is key in defining causal relation-
ships.Ifthispointofviewistaken(butnotifnot),Hollandappearsto
be conflating the distinct problems of defining and identifying causal
effects.

treatmentofcausalityishisclaimthat‘‘causalityisinthemind.’’This
claimstemsfrom(a)thefactthatcausaleffectsaredefinedaschanges
in outcomes when variables in a model are (hypothetically) manipu-
lated and (b) the view that models are mental constructs made up by
the scientist, ‘‘not empirical statements or descriptions of actual
worlds’’ (page 3). While Heckman’s conclusion is consistent with (a)
and (b), and Heckman is certainly free to define causality in this
fashion, I do not believe that most scientists (or philosophers)
would subscribe to this view, and were they to do so, they would
presumablyhavelittlefurtherinterestincausality(assciencetypically
purports to be concerned with the real world).

In this vein, models (be they mathematical or of some other
sort) are often constructed by scientists to represent causal processes
(causalmechanisms)believedtobeoperating inthe actualworld(not
justthemind).Tobesure,themodelshavetobeimaginedandinthis
sense, our notion of the causal process(es) at play comes from the
mind, but the processes (which wemay or may notaccurately model)
are also believed to reside in the actual world. That is, the causal
relation is typically held to describe a relation that is believed to exist
in the real world.

3. DEFINING CAUSAL ESTIMANDS
In the statistical literature on causal inference, as in Heckman,
assumptions (A-1) and (A-2) are typically made; Rubin (1980) has
called this the stable unit treatment value assumption (SUTVA).

Whentheseassumptionsarenotmade,theproblemofdefiningcausal
estimands is more difficult, as is the problem of making inferences
about these. In addition to Heckman, several others have worked on
this problem (Halloran and Struchiner 1995; Sobel 2001, 2003). But
this is fertile ground for social scientists, where interference due to
social interactions and other constraints are the norm. Nevertheless,
following Heckman, I shall hereafter assume SUTVA holds.

With (A-1) and (A-2) in hand, the response of unit ! to level s
ofthecausemaybewrittenasY(!);forthepurposesathand,assume
s
that each unit can take on every level of the cause. Individual (unit)
causal effects are then defined as an intra-unit, between-treatment
observed only under one treatment, it is not possible to observe unit
causal effects. Holland (1986) refers to this fact as ‘‘the fundamental
problem of causal inference.’’
Heckman focuses attention on three estimands in this paper,
theaveragecausaleffect(ACE),theeffectoftreatmentonthetreated
(TT), and the marginal treatment effect (MTE). Although the MTE
can be useful for understanding other estimators, I do not discuss it
further herein, asI believe sociologists will usually be more interested
in the other two estimands. The local average treatment effect
(LATE) will be discussed subsequently.

LetSdenoteasetoftreatmentsofinterest.Theaveragecausal
effect of treatment s versus s0 (ACE(s, s0)) is defined as
EðY (cid:4)Y Þ; ð2Þ
s s0
in which h(Y (!), Y (!)) ¼ (Y (!) (cid:4) Y (!)). The ACE can also be
s s0 s s0
definedconditionallyoncovariatesW;asinHeckman,thisisdenoted
ACE(s, s0 j W), and when it is obvious which treatments are being
compared (as in the case where there is just one treatment compared
tonotreatment)simplyACE(W),orACEinthecasewherethereare
no covariates. Since, following Heckman, Y (!) is defined as the out-
s
come of unit ! when treatment s is received, herein the ACE is the
average difference when all units receive treatment s as versus s0. It is
also(byvirtueofassumptions(A-1)and(A-2)),theeffectofreceiving
treatment s versus s0 for a randomly selected person from the
population.

Theaverageeffectoftreatmentsversuss0onthetreated(TT(s,s0))
is another parameter of longstanding interest:
EððY (cid:4)Y ÞjD¼sÞ; ð3Þ
s s0
where D is the random variable denoting which treatment in S is
actually received. Thus, TT(s, s0) is the average effect of treatment s
versus s0 for those units that actually take up treatment s.

To round out the discussion, I also want to consider a para-
meter that has received a great deal of attention from biostatisticians
and the public health community, the so called ‘‘intent to treat’’
estimand (ITT(s, s0)). For all s 2 S, we define Y~(!) as the outcome
s
subject is assigned may differ from the treatment received because
subjects will not always take up the treatment to which they are
assigned; thus Y~(!) 6¼ Y(!) in general.) ITT(s, s0) is then defined
s s
by (2) with Y~ and Y~ replacing Y and Y , respectively. Note that in
s s0 s s0
the case where all subjects would take up their assignments, for any
possible assignment, Y~(!) ¼ Y (!) and ITT(s, s0) ¼ ACE(s, s0).

s s
Therehasbeensomecontroversyoverwhichoftheparameters
aboveareofgreatestinterest.Itaketheviewthatitalldependsonthe
problem at hand, the goals of the scientist(s) analyzing the data and
the purposes of the person(s) making policy on the basis of the
analysis. Some examples where one or more of the parameters
above are of interest follow.

Forpolicieswithuniversalcoverageanduniversalparticipation,
theACEistheobviousparameterofinterest.Forexample,considerthe
effect of a specific currency devaluation (s ¼ 0 if no devaluation, 1
otherwise)onhouseholdspending.HereY~(!) ¼ Y(!)forall!,imply-
s s
ingACE ¼ ITT.Ifthedevaluationisimplemented,ACE ¼ TTaswell
(aseveryunittakesupthetreatment).

For policies with universal coverage that do not require parti-
cipation, some units may not take up the treatment. Because non-
participating units will not obtain the benefits of participation, it
might be argued that knowing the average effect of treatment for
these units is irrelevant, suggesting TT is the parameter of interest.

However, the untreated might take up treatment in the future if they
believed the treatment were effective (for them). Thus, we might wish
to also know the TUT (effect of treatment on the untreated).

Alternatively, policymakers might want to know the effect for the
nonparticipatingunits,forifthisisdeemedsubstantial,theywillthen
want to make efforts to obtain the participation of such units. They
will then also want to know the ACE, which is a weighted average of
the TT and the effect of treatment on the untreated (TUT).

But some might argue instead that the effect that should be of
interestistheeffectofofferingtheprogram.Forexample,considerthe
case of a new contraceptive method. Whereas some scientists may be
moreinterestedintheACE(orpossiblytheTT),whichmeasuresmore
directly the clinical effectiveness of the contraceptive, policymakers
considering whether or not to widely distribute the contraceptive in a
developing country are more concerned with the cost and the efficacy
instructions). Consequently, they are more interested in the ITT.

Finally, it is worth noting that if receipt of treatment is inde-
pendentofthepotentialoutcomes,givenasetofknowncovariatesW
(includingthe caseofnocovariates),TT(W) ¼ TUT(W) ¼ ACE(W).

Heckman also discusses a number of outcome measures that
may be of interest to social scientists and economists but which are
not discussed in the statistical literature he criticizes, where the out-
comes Y (!) are typically straightforward measures of the status of a
s
unit—for example, the income of a family under treatment s or the
survival time of a subject after surgery. In particular, Heckman con-
siders outcomes V(Y (!)) where V is some function of the outcome—
s
for example, the utility of Y (!) to individual ! (or to a policymaker)
s
under policy s. He then uses these to define various parameters
comparing the benefit (welfare) associated with alternative policies.

Although mathematically nothing new is involved here, this is useful,
especially because it is possible that E(V(Y ) (cid:4) V(Y )) (cid:5) 0 when
s s0
(2) > 0, for example. Thus, if V were to measure a social planner’s
utility, the planner would not wish to choose policy s over s0 even
thoughtheaveragecausaleffectisgreaterthan0.Choosingapolicyis
often not this simple, however; for some interesting recent work that
applies decision theory to the problem of treatment choice, see
Manski (2000, 2004).

The estimands above are differences between means. Because
the integral is a linear operator, these estimands only require knowl-
edge of the marginal distributions F(y) and F(y 0) of potential
s s
outcomes.Undersomecircumstances(discussedlater),thesedistribu-
tions can be identified.

(cid:2) (cid:3)
Heckmanalsodiscussesanumberofotherestimandsh Y ;Y0
s s
(cid:2) (cid:3)
of substantive interest that depend on the joint distribution F y ;y0
s s
of ðY ;Y Þ. However, the fundamental problem of causal inference
s s0
precludes the simultaneous observation of Y(!) and Y0ð!Þ, implying
s s
that it is not possible to know more than the marginal distributions.

And while knowledge of the marginal distributions imposes some
constraints on the joint distribution, these constraints often do not
allow much useful information on the joint to be extracted (for
example, if the marginals are normal with known means and var-
iances, this is consistent with non-normal joint distributions, as well
as a bivariate normal with any correlation between (cid:4)1 and 1). Thus,
estimate parameters depending on the joint distribution of potential
outcomes than parameters depending only on the marginal distribu-
tion of the potential outcomes (for an example of this, see Carneiro,
Hansen and Heckman 2003). Since the data impose few constraints
(as discussed above) and the joint distribution of potential outcomes
is not even an explicit auxiliary consideration in any substantive
theory I can think of, the possibility that mathematical assumptions
made primarily for the sake of convenience or tractability may be in
large measure generating the ‘‘empirical’’ results seems especially
strong here; sensitivity analyses should be a must.

4. POLICY EVALUATION AND FORECASTING
Drawing upon themes exposited at greater length in Heckman (2000,
2001) and several subsequent papers, Heckman emphasizes the value
of the ‘‘scientific approach’’ (as exemplified by structural models) for
policy evaluation and forecasting. He distinguishes three problems:
(1) evaluating policies that have been implemented, (2) extrapolation
of these to new environments, and (3) forecasting the effects of
policies that have not been implemented to new environments.

Heckman uses a structural equation model of the form (cid:2)(X(!),
U(!)) to examine this problem, writing the expectation of the
observedoutcome Y in the historical population, conditional on X as
Z
E ðYjX¼xÞ¼ (cid:2)ðx;uÞdF ðujxÞ; ð4Þ
H H
U
whereF (u j x)istheconditionaldistributionofUgivenX ¼ xinthe
H
historicalpopulation.Forproblem2,wewanttoknowE (Y j X ¼ x).

T
It is clear from equation (4) that this problem is easily solved if the
distribution F (u j X ¼ x) is known in the new environment (target),
T
assuming also the invariance of (cid:2) and the condition that the support
of(X,U)inthetargetpopulationiscontainedinthesupportof(X,U)
in thehistorical population.Of course,the assumptions andinforma-
tion needed to solve this problem are very strong. The third problem
canbedealtwith inasimilarfashion(see theappendixtoHeckman’s
paper), although it is more complicated.

inference has focused on estimating the impact of policies in a given
environment and problems 2 and 3 have not received much explicit
attention. But certainly problem 2 is easy to address within the usual
‘‘treatment effect’’ framework and perhaps this is why it has not been
addressed explicitly; problem 3 I discuss momentarily.

Inowproceedtodiscussproblem2withina‘‘treatmenteffects’’
framework forseveral reasons. First, I want the reader tounderstand
that the ‘‘treatment effects’’ framework and the ‘‘scientific’’ frame-
work, despite apparent differences, often yield very similar answers
torealquestions.Inparticular,thatmustbethecasewhenweseethat
the answers actually rest on similar assumptions, once these are
exposited. Second, in comparing Heckman’s structural approach
with the alternative I exposit below, I believe that some researchers
who might need to address this problem in their future substantive
work may find it easier to think about this problem from the ‘‘treat-
ment effects’’ perspective.

For the sake of concreteness, consider the problem of extra-
polating the historical ACE E (Y (cid:4) Y ) to a new population
H 1 0
T. The obvious thing to do is to think of a set of covariates Z such
that the historical and target ACEs are identical, and to average the
historical ACE over the marginal distribution of Z in the target
population.

More formally (assuming Y and Y are real valued scalars),
1 0
the conditional ACE in the target population is
Z Z
E ððY (cid:4)Y ÞjZ¼zÞ¼ y dF ðy jzÞ(cid:4) y dF ðy jzÞ: ð5Þ
T 1 0 1 T 1 0 T 0
R R
Knowledge of the target distributions F (y j z) and F (y j z) is
T 0 T 1
sufficient to determine the value of equation (5); of course, the pro-
blem is that target distributions are unknown and it might be very
difficulttospecifythem.Thesimplestthingistoassumethehistorical
and target distributions are the same (where these are both defined)
F (y j z) ¼ F (y j z) for s ¼ 0, 1. Alternatively, in this case, we
H s T s
might just as well assume the weaker condition E ((Y (cid:4) Y ) j
T 1 0
Z ¼ z) ¼ E ((Y (cid:4) Y ) j Z ¼ z). Either of these is an invariance
H 1 0
assumption and should not be lightly made. But this characterization
and this should allow an investigator to think reasonably about the
necessary components of Z. Continuing, the target ACE is then
Ð
E (Y (cid:4) Y ) ¼ (E ((Y (cid:4) Y ) j z)dF (z). Of course, to average
T 1 0 R H 1 0 T
the integrand over the target distribution, it must be defined for all
values (up to a set of probability measure 0) that Z takes on in the
target population. This will be the case, for example, when the sup-
ports of (Y, Z) in the target population are contained in the supports
s
of (Y, Z) in the historical population.

s
Now suppose Y is, as in Heckman, the invariant (over the
s
historical and target population) structural equation; Y ¼ (cid:2) (Z, U )
s s s
for s ¼ 0, 1, then
Z
E ðY jZ¼zÞ¼ (cid:2) ðz;u ÞdF ðu jzÞ: ð6Þ
T s s s T s
R
Analogous to the case above, if the target distributions
F (u j z) and F (u j z) are known, the value of (6) is known. If
T 0 T 1
these are assumed to be identical to their historical counterparts,
thisimpliesF (y j z) ¼ F (y j z)fors ¼ 0,1; ifsomeotherassump-
H s T s
tion is made, this cannot be the case. Note also that
F (y j z) ¼ F (y j z) for s ¼ 0, 1 does not imply invariance of the
H s T s
structural equation model nor the conditional distributions of U. As
s
before, the ACE is obtained by averaging over the marginal distribu-
tion of Z in the target population.

As for the second point, I at least find it easier to think about
the distributions F (y j z) (or the conditional ACE) than to think
T s
aboutinvariantstructuralequationsandtheconditionaldistributions
of the unobservables. (Of course, this does not invalidate a structural
approach.)
Heckman is also very critical of the ‘‘treatment effects litera-
ture’’ for its failure to deal with problem P3, and he briefly (see some
of Heckman’s more recent work with Vytlacil for a more detailed
treatment) considers this problem here, suggesting that treatments be
viewed as a bundle of characteristics. The relationship between these
characteristics (as versus just the treatments themselves) and the
response (possibly with covariates) can then be modeled, and the
relationship transported to the new environment, as per problem P2.

which makes one wonder why statisticians have not addressed this
topic. In that regard, several points are in order.

First, formorethan75years, statisticiansand appliedworkers
have been using factorial experiments in conjunction with Fisher’s
analysis of variance (and more generally, response surface meth-
odology) to both identify and estimate the effects of the factors
(characteristics) comprising the treatment on the response, and to
extrapolate these to conditions not actually experienced. A simple
example is a partial factorial design, where higher order interactions
are assumed to be 0, allowing extrapolation to combinations of the
components not actually observed.

The solution above to problem P3 will be inadequate when
the effects of the factors vary by covariates whose distributions are
different in the historical and target population. In this case, it
would be necessary to estimate the effects conditionally and then
average over the distribution of these in the target population, as
above. Conceptually, this is straightforward. Practically, the pro-
blem is to know what covariates to use and the relationship
between the effects of the factors and the covariates. In the sim-
plest case, where the investigator really does know what covariates
to use and the covariates take on only a few levels, it may not be
necessary to introduce (possibly arbitrary) modeling assumptions
about the relationship between the effects of the factors and the
covariates to make headway. But when there are many covariates
and/or several continuous covariates, such assumptions become
necessary.

There are two matters that make for additional complexity
and the need for yet more assumptions. In observational studies,
treatment assignment may not be ignorable. If it is ignorable, given
known covariates, one can (in theory) proceed as above. If not, other
avenuesmustbeconsideredtoachieveidentificationofparametersof
interest.

Finally and perhaps critically, in contrast to the case in the
experimental design literature, in most observational studies and
social experiments, the number of characteristics an investigator
would like to consider may far exceed the number of treatment
groups. This will make for identification problems and point identifi-
cation may end up resting on a number of assumptions that are
observational study where it is reasonable to assume treatment assign-
ment is ignorable (without covariates) and the average effects do not
depend on covariates whose distributions differ in the historical and
target populations. In this case, transporting the relationship between
theresponseanditscomponentstothenewenvironmentissimple,once
therelationshipisdetermined.Supposenowthereare5components,each
having2values(i.e.,thereare32combinationsofcomponentvalues);to
identifyalltheeffects inthemostgeneral case, 32treatmentgroupsare
needed.Evenifmanyofthehigherorderinteractionsdisappear,identi-
ficationproblemswillremainiftherearefewtreatmentgroups,asinthe
usual case. This maybe theprimary reason thatthe ‘‘treatment effects’’
literature has not explicitly unbundled the components of interventions
andattemptedtoaddressproblemP3initsfullgenerality.Thatsaid,itis
unfortunatethatsocialexperimentsarenotusuallydesignedtofacilitate
understandingtherelationshipbetweenthecomponentsandtheeffect.

5. IDENTIFYING AND ESTIMATING CAUSAL EFFECTS
5.1. Background
Since the invention of randomization (generally attributed to Fisher
[1925]), statisticians have emphasized the importance of study design
fortheestimationofcausaleffects.Inacompletelyrandomizedexperi-
ment—assumingrandomsamplingfromthepopulationofinterestand
(A-1)and(A-2)—theoutcomesofsubjectsassignedtoreceivetreatment
sarearandomsamplefromthedistributionofY~;thus,thisdistribution
s
canbeconsistentlyestimatedfromthedatacollectedintheexperiment.

Consequently,aspreviouslynoted,comparisonsofpotentialout-
comes that only require knowledge of the marginal distribution of out-
comescanbemadeinrandomizedexperiments.Forexample,statisticians
havetestedwhetherornottheoutcomeundertreatmentsisstochastically
higherthantheoutcomeundertreatments0.AnotherexampleistheITT.

Lettingsdenotetreatmentands0 thecontroltreatment,statisticianshave
longknownthatwhendataarecollectedusingrandomizedexperiments,
thedifference between the treatmentgroup mean andthe control group
meanontheoutcomeisanunbiasedestimateoftheITT.

Under complete randomization the set of potential outcomes
where A is the treatment assignment variable and the notation is
used to denote statistical independence. (Note that A refers to the
treatment assigned, which may not be the treatment actually
received.) Letting Y~ denote the observed response, under (7),
E(Y~ j A ¼ s) ¼ E(Y~ j A ¼ s) ¼ E(Y~); thus, the observable condi-
s s
tional expectations identify the parameter ITT(s, s0).

The completely randomized experiment is a special case of the
conditionally randomized experiment in which subjects are first
grouped according to a set W of pretreatment covariates, and a
completely randomized experiment is then conducted within the
groups. Under conditional randomization, treatment assignment is
‘‘ignorable’’ given the covariates W:
ðfY~ g ÞkAjW: ð8Þ
s s2S
Consequently, ITT(s, s0 j W) is identified from the observable condi-
tional expectations:
EðY~ (cid:4)Y~ jWÞ¼EðY~jA¼s;WÞ(cid:4)EðY~jA¼s0;WÞ: ð9Þ
s s0
Rubin (1977, 1978) saw that the conditionally randomized
study provides a means to bridge the gap between experimental and
observational studies. In observational studies, it is often not reason-
able to believe the ignorability assumption:
ðfY g ÞkD: ð10Þ
s s2S
However, if covariates W can be found that determine the treatment
receipt process (in the sense that given these covariates, receipt of
treatment does not depend on the potential outcomes), treatment
assignment is ignorable, given the covariates (Barnow, Cain and
Goldberger 1980 dubbed this ‘‘selection on observables’’):1
1Editor’s Note: This sentence is misprinted. The latter part of this
sentence should read as follows: ‘‘... treatment assignment is ignorable, given
the covariates (Barnow, Cain and Goldberger 1980). Heckman dubbed this
‘selection on observables’):’’ After this page was typeset and finalized, it was
discoveredthatthecharacterstring‘‘).Heckman’’wasinadvertentlyomitted.

s s2S
Under (11), the conditional means for treatments s and s0
identify ACE(s, s0 j W):
EðY (cid:4)Y jWÞ¼EðYjD¼s;WÞ(cid:4)EðYjD¼s0;WÞ: ð12Þ
s s0
Theintuitionbehind(11)isstraightforwardandreadilylendsitself
to use by empirical investigators. Within levels of W, treatment receipt is
decided (inthe binarycase) bythetossofa(possiblybiased) coin. Ifthe
parameter(2)isofinterest,asversus(12),thisisobtainedbyaveragingover
themarginaldistributionofW.Iftheaverageeffectisthesameforallvalues
ofW,itisnotnecessarytoknowthedistributionofW.Otherwise,itmustbe
possibletoestimatethisdistributionfromthedataorthedistributionmust
beknown;inpractice,itmaybethatneitheroftheseconditionsisattainable.

By way of contrast, despite a longstanding interest in making
causal statements, until more recently economists were less interested
in experimental data than statisticians. In part, this is due to the fact
that economists are interested in many questions that are not parti-
cularly amenable to experimentation.

Economists have also long recognized that human agents make
choicesandtheyusetheoriesofrationaldecisionmakingtocharacterize
the manner in which agents choose among alternatives. That is, econo-
mistsattempttocarefullyconsideronesetofmechanismsthatindividuals
might use to allocate themselves to treatments (how agents choose D).

Further,thisallocationprocessisoftenofintrinsicinteresttoeconomists.

Heckmancharacterizesthestatisticalliteratureasincomplete,in
part because statisticians do not model the allocation process. An
example of this is adjusting for covariates using regression analysis,
longadvocatedbystatisticians.Heremodelingtheconditionalexpecta-
tion E(Y j W, D) alone leads to an estimate of (12). If interest resides
solelyinestimating(12)when(11) holds,thereisnoneedtomodelthe
allocation process. But even when (11) holds, especially in an observa-
tional study where W may be a large vector, statisticians will often
advocate modeling the allocation process to reduce the dimensionality
of the estimation problem, a subject to which I shall return.

Nevertheless,thefocusinthestatisticalliteratureisprimarilyon
obtainingthebestpossibleestimateofthecausalparameterofinterest.

randomized experiment and an observational study where units select
their own treatment, the experiment is typically preferred (especially
when ITT is the parameter ofinterest and/or ACE is the parameter of
interest and subjects comply with their assignments (that is, for all !
and for all s 2 S, subject ! takes up assignment s when assigned to s).

Ingeneral,thewayinwhichunitsareallocatedintheexperiment
will not reflect the real-world allocation mechanism where human
agents are making choices, as studied by economists. As such, the
opportunity to learn about this mechanism (at least from the experi-
mentalstudy)isgivenup.Thisisthepricewepaytoensure(7)or(8).

In the observational study, however, we cannot be certain that
allrelevantcovariateshavebeentakenintoaccount.If(11)holdsand
the regression function is modeled correctly, we can learn about both
the allocation process and the causal parameter(s) of interest. But if
one or more covariates have not been taken into account and (11) is
assumed,credibleestimatesofcausalparametersmaynotbeobtained.

As Heckman points out, individuals making decisions may have rele-
vantinformationthatisnotaccessibletotheinvestigatorandtherefore
such information cannot be included in the investigator’s model of
the agent’s choice. In economic models of behavior, agents use this
‘‘hidden’’ information in computing the expected utility of different
choices. The agent then makes the choice that maximizes expected
utility. Since it is not unreasonable to suppose that utility is a mono-
tonefunctionofmanyofthetypesofoutcomes(forexample,earnings)
studied by economists, in such circumstances (11) will in general not
be satisfied for the set of pretreatment covariates accessible to the
investigator, and (12) will then not hold. In this case, if (11) is
(correctly) not assumed for a given set of available covariates W,
credibleestimatesmightbeobtainedusingothermethods—forexam-
ple, fixed effects models (including differences in differences), control
functions, instrumental variables. But if the assumptions underlying
the use of these alternatives are incorrect in the application under
consideration,thenasbefore,credibleestimatesmaynotbeobtained.

5.2. Matching, Control Functions, and Instrumental Variables
These are three approaches to estimating causal parameters.

Interestingly, although the rationale and assumptions needed to
figures prominently in all three.

In observational studies where it is believed that (11) holds,
therestillremainstheproblemofestimatingE(Y j W,D).WhenWis
a high-dimensional vector and/or several components have many
values, it may be difficult to specify the form of this function cor-
rectly, which can lead to faulty inferences. Matching will also be
problematic in this case.

Let S ¼ {0,1}. In a key paper, Rosenbaum and Rubin (1983)
showed that when (11) holds and
0<PrðD¼1jWÞ<1; ð13Þ
then
ðfY g ÞkDÞjPðWÞ; ð14Þ
s s2S
0<PrðD¼1jPðWÞÞ<1; ð15Þ
where P(W) ¼ Pr(D ¼ 1 j W) is the ‘‘so called’’ propensity score.

Imbens (2000) generalizes the notion of a propensity score to the
case of finitely many treatments. Imai and van Dyk (2004) extend
thenotionofapropensityscoretothemoregeneralcasewhereDmay
take on infinitely many values.

As a consequence of (15),
EðY (cid:4)Y ÞjPðWÞÞ¼EðYjD¼1;PðWÞÞ(cid:4)EðYjD¼0;PðWÞÞ: ð16Þ
1 0
Equation (16) provides the mathematical justification for matching on
the one-dimensional propensity score (as opposed to the multidimen-
sionalvectorW,whichmaywellbesparse),inwhichobservationswith
thesamevaluesofP(W)—one withD ¼ 1, the other withD ¼ 0—are
randomly paired, their difference providing an unbiased estimate of
(16).Anunbiasedestimateoftheparameter(2)canthenbeformedby
taking the appropriate weighted average. Equation (16) can also be
used to justify a related method called subclassification and to justify
covariance adjustment using only D and P(W) (as versus D and W).

Other parameters (for example, TT) can also be estimated using these
and related methods. The interested reader might wish to consult
Smith (1997) for a sociological application and Imbens (2004) for a
nice overview of estimating average treatment effects under the
assumption (11).

The beauty of matching is explained quite nicely by Heckman
(page65,thisvolume)andinHeckmanandNavarro-Lozano(2004:33):
matching ‘‘does not require separability of outcome or choice equa-
tions into observable and unobservable components, exogeneity of
conditioning variables, exclusion restrictions or adoption of specific
functionalformsofoutcomeequations.’’Othermethodsofestimating
causal effects, such as instrumental variables, fixed effects, and con-
trolfunctions,normallyrequireoneormoreassumptionsoftheform
above.

Nevertheless, Heckman is quite critical of matching on the
propensity score. First, the method breaks down if P(W) ¼ 0 or 1
for one or more values of W. In practice, even in less extreme cases,
an investigator may encounter the case where the estimated P(W) is
close to, for example, 1 and there are no ‘‘good’’ matches from the
control group. When such data are excluded, as is often the case, the
causal parameter that is actually estimated (an average effect on a
common support) may be of less interest. Second, when P(W) is
unknown (the typical case) and it is estimated nonparametrically,
the dimensionality problem is simply transferred to this estimation
problem.

Heckman also argues that it is often difficult to justify the use
of (11) for some conditioning set W. According to him, this situation
isexacerbatedbytheabsenceofanexplicitmodeloftreatmentchoice.

Finally, he states that (11) is quite strong substantively, implying
MTE(W) ¼ ACE(W) ¼ TT(W).

Ofcourse,itcanbearguedthat(14)mayholdevenif(11)does
not. But it is difficult to think of substantive situations where we
would want to argue that (14) holds and hence that (16) holds but
(11) does not. We should note also that (12) may hold even if (11)
does not hold, and that (16) can hold even if (14) does not. However,
as above, it is difficult to think of instances where we would want to
argue that one of the weaker conditions holds, but the stronger does
not. Thus, I do not consider it worthwhile to further entertain argu-
ments of this nature.

contexts.Hesuggeststhatwhenagentshavehunchesaboutthevalues
of the potential outcomes, and treatment choice is based on those
hunches, assumption (11) will not hold. While often true, there may
nevertheless be situations where an investigator knows and measures
the covariates on which the agents’ decisions are based, in which case
(11) holds. See also Imbens (2004) for less trivial examples.

When investigators do not think carefully about the treatment
assignment process in observational studies, they are likely to omit
important covariates from consideration. That said, it is not the
statistician’s job to substantively justify a particular model of choice.

Norwoulditbecorrecttosuggestthatstatisticiansareignorantof,or
do not stress the importance of understanding the treatment assign-
ment mechanism. Indeed, going back to Fisher (quoted in Cochran
1965) statisticians have long acknowledged the importance of having
a good theory of the treatment assignment mechanism; see also
Rosenbaum (2002, ch. 1), who pays a great deal of attention to this
matter.) Rosenbaum and others (see Rosenbaum [2002] for further
citations) have also studied the consequences due to the failure to
adjust for relevant omitted covariates.

Nevertheless, even when an investigator pays very close atten-
tion to the treatment assignment mechanism, a covariate (set of
covariates) known to be relevant may be missing from the data and/
orsomerelevantcovariatesareunknowntotheinvestigator.Thiswill
bethecaseinsomeinstanceswheretreatmentassignmentistheresult
of an economic agent behaving rationally and in other instances
where some other process describes the allocation to treatment
groups. Unfortunately, assumption (11) is not directly testable,
though it may be possible, by introducing auxiliary assumptions, to
test this indirectly. Heckman’s Tables 2 and 3 simply demonstrate
what they should: if the assumptions underlying the use of matching
are incorrect and the assumptions underlying Heckman’s particular
example of the use of control functions are correct, the observable
parameters that also equal TT and ACE in the case where matching
hold are now biased for TT and ACE. When it is suspected that (11)
does not hold, an investigator can attempt to conduct sensitivity
analyses (as statisticians have long advocated), construct bounds on
the parameter(s) of interest—for example, Manski (1990) and Robins
(1989)—or use some other approach—for example, fixed effects,
parameter of interest.

Following Heckman, I now examine the method of control
functions, expositing the additively separable case also considered
by him. He assumes (his equations 22a–22c)
V¼(cid:3) ðWÞþU ; EðU jWÞ¼0; ð17Þ
V V V
Y ¼(cid:3) ðXÞþU ; EðU jX¼0Þ; ð18Þ
s s s s
where s ¼ 0 or 1 and D ¼ 1 if and only if V > 0.

The observable conditional expectations (Y ¼ Y if D ¼ 1, Y
1 0
if D ¼ 0) are (using 18)
EðYjX;Z;D¼sÞ¼(cid:3) ðXÞþEðU jX;Z;DÞ: ð19Þ
s s
Under assumption (18), when (11) holds (with (X, Z) ¼ W),
E(Y j X, Z, D ¼ s) ¼ E(Y j X, Z) ¼ (cid:3)(X). Note that the first equality
s s
followsfrom(11)andthesecondfromtheadditionalassumption(18);that
is,theadditionalassumption(18)isnotneededtojustifymatchingonthe
propensityscore.Inthemethodofcontrolfunctions,however,assumption
(11) is not made and the components E(U j X, Z, D ¼ s) are modeled.

s
Notethat E(U j X, Z, D ¼ 1) ¼ E(U j X, Z, V > 0) ¼ E(U j X, Z,
1 1 1
U > (cid:4)(cid:3) (Z)) by virtue of assumption (17); similarly, E(U j X, Z,
V V 0
D ¼ 0) ¼ E(U j X,Z,V (cid:5) 0).Thus,under(17) and(18),itmightseem
0
that the method of control functions is more general than matching. But
modeling E(U j X, Z, D ¼ s) will require additional assumptions—for
s
example, Heckman’s assumption (C-1): (U , U , U )??(X, Z).

1 0 V
Assumption (C-1) implies (U , U ??(X, Z) j U , so that E(U j X, Z,
1 0 V s
D ¼ s)dependsonX,ZonlythroughthepropensityscoreP(X,Z).As
inmatching,aprobleminvolvinghighdimensionalityisnowreducedto
aone-dimensionalproblemthroughtheuseofthepropensityscore.Itis
worthnotingthatassumption[C-1]doesnotimply(11).Nordoes(11)
imply(C-1).Thus,evenif(17)and(18)hold,itisnotthecasethat‘‘the
controlfunctionapproachismoregeneralthanthematchingapproach’’
(page 73, this volume). (Heckman points out that assumption (C-1) is
not essential. Nevertheless, if this assumption is removed, others will
tionsandwillthusbeusefulindifferentcircumstances.

One other point should be made. Heckman notes: ‘‘Without
invoking parametric assumptions, the method of control functions
requires an exclusion restriction (a variable in Z that is not in X) to
achieve nonparametric identification.’’ But he is far less critical of
these assumptions (and others noted above) than he is of those
required to justify matching and the use of instrumental variables.

In that vein, Vella (1998, p. 131) points out the sensitivity to para-
metricassumptionsofHeckman’soriginalwork:‘‘Asestimationrelies
heavily on the normality assumption, the estimates are inconsistent if
normality fails.’’ Vella (1998, p. 135) also notes that the exclusion
restriction is ‘‘controversial’’ and he argues that many theoretical
economic models of behavior, including the Roy model discussed by
Heckman, explicitly impose Z ¼ X.

Usinginstrumentalvariablesisanotherwaytoestimatetreatment
effectsinobservationalstudies,anditmakesassumptionsthataredifferent
than those made in matching or the method of control functions.

Social scientists have long used instrumental variables to estimate treat-
ment effects when treatment choice is ‘‘endogenous.’’ Traditionally, the
techniqueisexpositedasfollows.Considertheregression
Y¼(cid:3)ðXÞþ(cid:4)Dþ"; ð20Þ
where D ¼ 1 if the treatment is received, 0 otherwise, (cid:4) is the desired
treatment effect, and E(" j X) ¼ 0. The problem here is that D is
correlated with ", so in general E(" j X, D) 6¼ 0 (equivalently,
E(Y j X, D) ¼ (cid:3)(X) þ (cid:4)D þ E(" j X, D)). However, if a variable Z
canbeobtainedthatisassociatedwithYonlythroughD,i.e., Zdoes
not directly affect the outcome, E(" j X, Z) ¼ E(" j X) ¼ 0, in which
case E(Y j X, Z) ¼ (cid:3)(X) þ (cid:4)E(D j X, Z). Consequently (assuming
E(D j X, Z ¼ 1) (cid:4) E(D j X, Z ¼ 0) 6¼ 0),
EðYjX;Z¼1Þ(cid:4)EðYjX;Z¼0Þ
(cid:4) ¼ : ð21Þ
EðDjX;Z¼1Þ(cid:4)EðDjX;Z¼0Þ
From a causal standpoint, the formulation above is quite
vague. Heckman has helped to clarify the literature on instrumental
variables. Angrist, Imbens, and Rubin (1996) is another paper that I
Heckman’s.Thus,Ibrieflyexpositthisapproachandsubsequentlytie
it to the exposition in Heckman; see also Vytlacil (2002).

I will focus on several parameters discussed by Heckman
(ACE(X)), the local average treatment effect (hereafter LATE(X)),
TT(X), and I will also briefly discuss ITT(X). Following Heckman,
Z is the instrumentalvariable. It will also betaken tobe binary,as in
Angrist et al. (1996). (See Angrist and Imbens [1995] for some gen-
eralizationsofthesetupconsideredherein.)LetZ(previouslydenoted
A)denotethetreatmenttowhichasubjectisassigned(0ifassignedto
the control group, 1 if assigned to the treatment group). Let D(!)
denotetheobservedchoiceofunit(!)andletD (!)denotethechoice
z
unit ! makes when assigned to treatment z 2 {0, 1}. Similarly, let
Y (!) denote the response of unit ! when that unit is assigned to
(z,Dz)
treatment z and chooses outcome D (!). (Previously, Y (!) was
z (z,Dz)
denoted Y~ (!).) Let Y (!) denote the outcome of unit ! when that
z zs
unitisassignedtotreatmentzand‘‘takesup’’treatments,forz ¼ 0,1,
s ¼ 0, 1. Note that for each assignment, individuals take up only one
treatment; nevertheless, as above, potential outcomes assuming they
had taken up the treatment they did not take up can be defined.

To begin, it is useful to formalize the exclusion restriction—
that is, the idea that the instrumental variable only affects the out-
come by affecting D. This is the assumption (Holland 1988)
Y ð!Þ¼Y ð!Þ ð22Þ
ð0;sÞ ð1;sÞ
for s ¼ 0, 1 and all !. Consequently, the potential outcomes may be
writtenasY(!).Theexclusionrestrictionisverystrong,anditcanbe
s
quite difficult to find instruments that satisfy this assumption.

TheproblemwithestimatingtheeffectofD(conditionalonthe
covariates X) on the outcome is that (11) will not generally hold,
becauseDis‘‘endogenous’’;thus,ingeneral,E(Y j D ¼ s,X) ¼6 E(Y j X).

s
However,if(8)holds(withZinplaceofA),aswouldbethecaseina
randomized experiment,
EðYjZ¼1;XÞ(cid:4)EðYjZ¼0;XÞ¼EðY (cid:4)Y jXÞ; ð23Þ
1;D1 0;D0
that is, ITT(X) is the numerator of the IV estimand (21). (Recall the
previous discussion, which suggests that at least in some instances,
policymaker.)
Continuing, ITT(X) may be broken down into the following
four components:
EðY (cid:4)Y jXÞ¼EEððY (cid:4)Y ÞjD ;D ;XÞ; ð24Þ
1;D1 0;D0 1;D1 0;D0 0 1
where (D , D ) ¼ (0, 0) or (0, 1) or (1, 0) or (1, 1). By virtue of the
0 1
exclusion restriction (22), units who always take up the treatment
(D (!) ¼ D (!) ¼ 1), hereafter called ‘‘always takers,’’ or never take
0 1
up the treatment (D (!) ¼ D (!) ¼ 0), hereafter called ‘‘never
0 1
takers,’’ contribute nothing to (24). Angrist et al. (1996) call subjects
with D ¼ 1, D ¼ 0 compliers and subjects with D ¼ 0, D ¼ 1
1 0 1 0
defiers; only these two types of units contribute to (24) under the
exclusion restriction.

Angrist et al. (1996) also assume there are no defiers (the
monotonicity assumption), in which case
ITTðXÞ¼EððY (cid:4)Y ÞjD ¼0;D ¼1;XÞPrðD ¼0;D ¼1jXÞ: ð25Þ
1;D1 0;D0 0 1 0 1
Dividing ITT(X) by the compliance probability (assuming this
is greater than 0) gives the parameter LATE(X), the average treatment
effect for the compliers (at X). The compliance probability Pr(D ¼ 1,
1
D ¼ 0 j X) > 0 may also be written (under the assumptions
0
here) as E((D (cid:4) D ) j X). But this is equal to E(D j X, Z ¼ 1) (cid:4)
1 0
E(D j X, Z ¼ 0) when treatment assignment (Z) is ignorable, given X,
as here. Thus, under the assumptions above, LATE(X) ¼ IV(X). Note
alsothatthecomplianceprobabilitymaybewrittenasPr(D ¼ 1jX)
1
(cid:4) Pr(D ¼ 1 j X) ¼ P(X, 1) (cid:4) P(X, 0), which makes the connection
0
withthepropensityscoreevident.

The parameter LATE(X) (or LATE when there are no covari-
ates X) will not always have policy implications of interest. To begin,
thecompliersconstitutealatentsubpopulation.So,evenifwewanted
toadministerthetreatmentonlytothecompliersanditwaspolitically
feasible to do so, it is not possible to identify these individuals (in
practice, we could model the probability of being a complier and
administer the program to those deemed ‘‘most likely’’ to be
compliers). Second, when the compliers are a ‘‘small’’ fraction of the
population, it may be difficult to argue that the results are of great
istheexcesscivilianmortality(between1974and1983)resultingfrom
service in the Vietnam War (not the excess mortality among
compliers). For men born in 1950, the compliers constitute only 15.9
percent of the population; technically, LATE only applies to this
fraction of the population. In some applications, however, even if
the compliers are a small fraction of the population, LATE (or
LATE(X)) is nevertheless a parameter of great interest. This would
be the case when it could be argued that the noncompliers, had
they complied, would experience the same benefits as the compliers.

I return to this subject momentarily. Third, Heckman (1997) has also
pointed out that LATE (LATE(X)) is an unusual parameter, insofar
as its very definition depends on the instrumental variable chosen.

Thus,insomecases,LATE(X)and/orLATEmayidentifyaparameter
with policy relevance (as when Z represents assignment under a parti-
cular policy of interest), and in other cases it may not. For further
discussion of LATE and other possible parameters of interest, see the
discussion following Angrist et al. (1996) and Heckman (1997).

AlthoughtheparametersLATEandLATE(X)maynotalways
be of great substantive interest, the methodological point is that the
meaning of the IV estimand has been clarified (which has great
substantive implications). In particular, a basis is provided that
makes it very easy to ask if IV(X) identifies other parameters of
possibly greater interest, such as TT(X) and ACE(X).

Toseethis,considertheparameterTT(X),whichconditionson
receipt of treatment (D ¼ 1). The units receiving treatment are the
compliersinthetreatmentgroupandthealwaystakers(stillassuming
there are no defiers). It follows from the foregoing results that
IV(X) 6¼ TT(X) in general, and that IV(X) ¼ TT(X) if and only if
the average effect of receiving treatment for the always takers
(assuming the probability of being an always taker is greater than 0)
and compliers is the same. Put this way, an analyst can ask whether
the equality of treatment effects across these two groups is a reason-
ableassumptiontomake.Iftheanalystsuspects,forexample,thatthe
always takers know that (even after conditioning on X) they will
benefit by taking up the treatment (or have higher gains than others
bysodoing),heorshewillnotwanttoassumeequalityacrossgroups
and hence that IV(X) ¼ TT(X).

IV(X) must equal TT(X). If the treatment cannot be obtained in the
control group, as in many social programs, it is not possible to be an
alwaystaker.Inthiscase,LATE(X) ¼ TT(X)(withoutitbeingneces-
sary to assume that the average effects of receiving treatment are the
same for compliers and always takers), hence IV(X) ¼ TT(X).

Similarly,iftheaverageeffectofDontheresponseisthesame
for compliers, always takers, and never takers, IV(X) ¼ LATE(X) ¼
TT(X) ¼ ACE(X). If it is not possible to be an always taker,
LATE(X) ¼ TT(X) (as above) and LATE(X) ¼ ACE(X) (hence
IV(X) ¼ ACE(X)) when it is assumed that the average effects of
receiving treatment are identical for never takers and compliers. In
cases where it is impossible to be a never taker (programs with
universal coverage and participation), LATE(X) ¼ ACE(X) if it is
assumed that the average effects of D on Y are identical for always
takers and compliers.

InthecasewheretheuniteffectsofDonYarethesameforall
!, the average effects of receiving treatment must be the same for all
units, hence all groups, implying IV ¼ TT ¼ ACE. Of course, the
assumption of constant effect is quite strong and not likely to be
substantively reasonable in most social science applications.

Finally, if the probability of being a defier is nonzero, in
general IV(X) 6¼ LATE(X); but in the special case where the average
effect of receiving treatment for compliers and defiers is the same,
IV(X) ¼ LATE(X).Angristet al.(1996)alsodiscusstheconsequences
of violating the exclusion restriction, and there is some literature on
estimating complier average causal effects in the absence of this
restriction (for example, see, Jo [2002]).

Heckman approaches this subject somewhat differently. He
imposes the additively separable model (18) on the potential out-
comes. He then writes the observed outcome Y in terms of the
potential outcomes as
Y¼(cid:3) ðXÞþð(cid:3) ðXÞ(cid:4)(cid:3) ðXÞþU (cid:4)U ÞDþU ; ð26Þ
0 1 0 1 0 0
expresses the parameters TT(X) and ACE(X) in terms of (26), and
states identifiability conditions in terms of D, U , and U .

0 1
The assumption of a constant effect holds (Y (cid:4) Y is the same
1 0
for all units) if U ¼ U (cid:3) U. In this case, the instrumental variable Z
0 1
under(18)E(Y j X,Z) ¼ E(Y j X)fors ¼ 0,1)).Asabove,asufficient
s s
condition for this is YkZ j X for s ¼ 0, 1, and as above, assuming
s
P(X,1) (cid:4) P(X,0) ¼6 0,IV(X) ¼ LATE(X) ¼ TT(X) ¼ ACE(X).

When the constant effect assumption fails but E(U (cid:4) U j X,
1 0
D ¼ 1) ¼ 0, Heckman (1997) shows that TT(X) ¼ ACE(X). A suffi-
cient condition for this is
ðU (cid:4)U ÞkDjX ð27Þ
1 0
(or more generally (Y (cid:4) Y )D j X). Though weaker than the condi-
1 0
tion (11), which was not presumed to hold, Heckman points out that
the sufficient condition above is nevertheless quite strong, requiring
thatreceiptoftreatmentnotdepend,givenX,ongainsanticipatedby
subjects. That is, in general, we should not expect TT(X) ¼ ACE(X).

We can also see this from the results above, where it was established
thatiftherearenodefiers,andtheaverageeffectofDontheresponse
is identical for compliers and always takers, LATE(X) ¼ TT(X) ¼
ACE(X).Similarly,iftherearedefiersandtheaverageeffectofreceiving
treatmentisidenticalfordefiersandcompliers,andforcompliersand
always takers, LATE(X) ¼ TT(X) ¼ ACE(X).

Heckman also gives general conditions under which IV(X) ¼
TT(X) and IV(X) ¼ ACE(X). As in the simpler case above, and for
the same reasons, Heckman argues that these conditions are quite
strong.Again,thisargumentseemsmostcompellingwhentheanalyst
doesnothave accesstodatathat thedecision makerisusing tomake
his decision and this information is predictive of the potential out-
comes.Forfurtherdetails,thereadermayconsultHeckman(1997)or
his paper in this volume.

6. CONCLUSION
Heckman argues for the use of an approach to causal inference in
which structural models play a central role. It is worth remembering
thatthesemodelsareoftenpowerfulinpartbecausetheymakestrong
assumptions. When these assumptions are correct, powerful (and
correct) inferences may be obtained. Such inferences are likely to be
stronger than those that would be made by advocates of randomized
tionalstudy,wemightlearn aboutthetreatmentassignment mechan-
ism and various average effects, and we might extrapolate the results
to a new policy in a new environment. But when the assumptions are
arbitrarily invoked in applications or require the use of knowledge
thattheinvestigatordoesnothave,asseemsoftenthecase,soarethe
inferencesderivedfromsuchmodelingexercises.Thus,aninvestigator
might well prefer to stick with simple estimators from randomized
experiments,wheneverpossible.Insuchacase(presumingtheexperi-
ment did not get botched and subjects complied with experimental
protocols), the investigator can have greater confidence in his or her
estimates of parameters such as ITT and ACE, for example.

ButIdonotwanttoarguethatstructuralmodelingisnotuseful,
nor do I want to suggest that methodologists should bear complete
responsibility for the use of the tools they have fashioned. To my
mind, both structural modeling and approaches that feature weaker
assumptions have their place, and in some circumstances, one will be
moreappropriatethantheother.Whichapproachismorereasonablein
a particular case will often depend on the feasibility of conducting a
randomizedstudy,whatwecanactuallysayaboutthereasonablenessof
invoking various assumptions, as well as the question facingthe inves-
tigator (which might be dictated by a third party, such as a policy-
maker). An investigator’s tastes and preferences may also come into
play. A cautiousandrisk-averse investigator maycareprimarilyabout
beingright,evenifthislimitstheconclusionsheorshedraws,whereas
another investigator who wants (or is required) to address a bigger
questionmayhave(orneedtohave)agreatertoleranceforuncertainty
aboutthevalidityofhisorherconclusions.

In his introductory section, Heckman claims to make two
major points: (1) that ‘‘causality is a property of a model of hypothe-
ticals’’ (page 2), and (2) that statisticians have conflated the distinct
tasksofdefiningparametersofinterest,identification,andestimation.

Ihavealreadydiscussedthefirstpoint.Iconcludewithadiscussionof
thesecond.Withrespecttothispoint,Heckmanwrites(page5):‘‘This
emphasisonrandomizationoritssurrogates(likematching)rulesout
a variety of alternative channels of identification of counterfactuals
from population or sample data. It has practical consequences
because of the conflation of step one with steps two and three in
Table 1. Since randomization is used to define the parameters of
zation is the only way—or at least the best way—to identify causal
parameters from real data.’’
Heckman appears to be arguing here that statisticians are
putting the cart before the horse by focusing interest on average
causaleffectsthatdonotdependonthejointdistributionofpotential
outcomes and emphasizing identification conditions in observational
studies that parallel random assignment, thus justifying estimation
methods such as matching and even randomization itself. While it is
impossible to assess such a claim, it is worth noting that average
causal effects such as the ACE and ITT have been of great interest
in public health, for example, for many years. These parameters can
and have been used to address policy questions that are of great
interest. Recall also that both the potential outcomes notation and
the ACE (Neyman 1923) preceded randomization.

Of course, Heckman is certainly correct to note that there are
interestingestimandsthatdependonthejointdistributionandthathere,
randomization is of considerably less help. In addition, as he and many
othershavepointedout,whenitisimpossiblefortheinvestigatortoobtain
a sufficiently rich set of covariates to condition on, other methods of
identifying,andestimatingcausaleffects(includingtheusualeffectsthat
donotdependonjointdistributionsofpotentialoutcomes)mustbeused.

But Heckman goes much further, arguing that statisticians
have confounded the tasks of defining, identifying, and estimating
causal parameters and, as above, even use randomization to define
parameters of interest. By and large (except for some minor quibbles
one might have about the way some authors have defined LATE), I
would argue the opposite. One of the key contributions that statisti-
cianshavemadeistounconfoundtheseissues,pavingthewayfor(1)
the assessment of conditions under which valid causal inferences are
permitted and (2) the development of appropriate methods for
making valid causal inferences.

Consider the claim that randomization is used to define causal
parametersofinterest.Intheintroduction,Istressedtheimportanceof
good notation. By using the potential outcomes notation, statisticians
(recall Neyman 1923 and later Rubin) were able to define causal esti-
mandsthatmirroredtheirthinkingonthecounterfactualnatureofthe
causal relation and that were different from the usual descriptive
(observable)parameters.

under what conditions various observable parameters are equal to
(identify) theseestimands.Randomizationisadeviceforassigningsub-
jectstotreatmentsthatmakestheignorabilityassumptions(conditions)
(8)and/or(11)plausible.Whentheseconditionshold,variousobservable
parameters also equal the causal estimands. These conditions may also
be met when randomization has not been used. This demonstrates the
logical independence between the ignorability conditions and randomi-
zation.Andclearly,theseconditionsarealsologicallyindependentofthe
definitionsofcausalestimandssuchastheITT,ACE,andTT.(Readers
mightalsowanttolookdirectlyatthedefinitionoftheseparametersand
notethatnomentionofrandomizationismade.)
More generally, defining causal estimands independently of the
conditions that must be met in order to identify them allows for the
development of appropriate procedures (including randomization,
matching, IV, control functions, etc.) for identifying (and then
estimating) the causal parameters. This is the approach taken in both
the‘‘treatmenteffects’’literatureandrecenteconometricliteratures,and
itisalsotheapproachthatHeckmantakes.Itisabigstepforward.

Another way to see the utility of making the definitions of
causal effects logically independent of the conditions needed to iden-
tify them is to consider the usual approach to regression analysis (or
structural equation models) which is typically taken (both in the past
and often even now) by many social scientists. The parameters of a
regression are certainly interpretable in a descriptive sense, but social
scientistsoftenimpartacausalinterpretationtooneormore(oftento
all) parameters, which are typically interpreted as ‘‘effects’’ in this
counterfactual sense (see Sobel [1990] for more on this point).

Justifications for such interpretations have included the notion that
the model is well specified and/or that important confounders have
beencontrolledand/orthatthecausalorderingiscorrect.Allofthese
justifications are extra-mathematical and virtually impossible to eval-
uate, insofar as a target (i.e., a well-defined estimand) has not even
been defined. Using an appropriate notation allows the researcher to
clearly define the estimand of interest independently of the regression
parameter(s), enabling the analyst to give conditions under which the
regression parameter(s) actually identify the target(s) of interest.

Although I disagree with him on this point and a number of
others, Heckman, in conjunction with his collaborators, has made
next generation of researchers will cooperate and incorporate the
various literatures on causal inference, including the statistical and
econometric literatures, under one umbrella. Science will be better
served when this is the case.

REFERENCES
Angrist, Joshua D., and Guido W. Imbens. 1995. ‘‘Two Stage Least squares
Estimation of Average Causal Effects in Models with Variable Treatment
Intensity.’’JournaloftheAmericanStatisticalAssociation90:431–42.

Angrist,JoshuaD.,GuidoW.Imbens,andDonaldB.Rubin.1996.‘‘Identification
ofCausalEffectsUsingInstrumentalVariables’’(withdiscussion).Journalof
theAmericanStatisticalAssociation91:444–72.

Barnow,BertS.,Cain,GlennC.,andArthurS.Goldberger.1980.‘‘Issuesinthe
AnalysisofSelectivityBias.’’Pp.43–59inEvaluationStudiesReviewAnnual,5,
editedbyE.StromsdorferandG.Farkas.BeverlyHills:Sage.

Bunge,Mario.1979.CausalityandModernScience.3ded.NewYork:Dover.

Carneiro,Piedro,Hansen,KarstenT.,andJamesJ.Heckman.2003‘‘Estimating
Distributions of Treatment Effects With an Application to the Returns to
SchoolingandMeasurementoftheEffectsofUncertaintyonCollegeChoice.’’
InternationalEconomicReview44:361–432.

Cochran, William G. 1965. ‘‘The Planning of Observational Studies of Human
Populations.’’JournaloftheRoyalStatisticalSociety,Series.A,128:234–55.

Collingwood, Robin G. 1940: 1972. An Essay on Metaphysics. Chicago, IL.:
HenreyRegneryCompany.

Cox,DavidR.1958.ThePlanningofExperiments.NewYork:Wiley.

Fisher, Franklin M. 1970. ‘‘A Correspondence Principle for Simultaneous
EquationModels.’’Econometrica38:73–92.

Fisher, Ronald A. 1925. Statistical Methods for Research Workers. Edinburgh,
Scotland:OliveandBoyd.

Halloran, M. E., and C. J. Struchiner. 1995. ‘‘Causal Inference in Infectious
Diseases.’’Epidemiology,6:142–51.

Heckman, James J. 1997. ‘‘Instrumental Variables: A Study of Implicit
Behavioral Assumptions Used in Making Program Evaluations.’’ Journal of
HumanResources32:441–62.

———. 2000. ‘‘Causal Parameters and Policy Analysis in Economics: A
TwentiethCenturyRetrospective.’’QuarterlyJournalofEconomics115:45–97.

———.2001.‘‘MicroData,Heterogeneity,andtheEvaluationofPublicPolicy:
NobelLecture.’’JournalofPoliticalEconomy109:673–748.

Instrumental Variables, and Control Functions to Estimate Economic
ChoiceModels.’’ReviewofEconomicsandStatistics86:30–57.

Holland, Paul W. 1986. ‘‘Statistics and Causal Inference’’ (with discussion).

JournaloftheAmericanStatisticalAssociation81:941–70.

———.1988.‘‘CausalInference,PathAnalysis,andRecursiveStructuralEquations
Models.’’(withdiscussion).Pp.449–493inSociologicalMethodology,editedby
C.C.Clogg.Washington,D.C:AmericanSociologicalAssociation.

Imbens,GuidoW.2000.‘‘TheRoleofthePropensityScoreinEstimatingDose-
ResponseFunctions.’’Biometrika87:706–10.

———. 2004. ‘‘Nonparametric Estimation of Average Treatment Effects Under
Exogeneity:AReview.’’ReviewofEconomicsandStatistics86:4–29.

Imai, Kosuke, and David A. van Dyk. 2004. ‘‘Causal Inference with General
TreatmentRegimes:GeneralizingthePropensityScore.’’JournaloftheAmerican
StatisticalAssociation99:854–66.

Jo, Booil. 2002. ‘‘Estimation of Intervention Effects with Noncompliane:
Alternative Model Specifications’’ (with discussion). Journal of Educational
andBehavioralStatistics27:385–420.

Kempthorne,Oscar.1952.TheDesignandAnalysisofExperiments.NewYork:Wiley.

Manski, Charles F. 1990. ‘‘Nonparametric Bounds on Treatment Effects.’’
AmericanEconomicReviewPapersandProceedings80:319–23.

———. 2000. ‘‘Identification Problems and Decisions Under Ambiguity:
Empirical Analysis of Treatment Response and Normative Choice of
TreatmentChoice.’’JournalofEconometrics95:415–42.

———. 2004. ‘‘Statistical Treatment Rules for Heterogeneous Populations.’’
Econometrica72:1221–46.

Neyman,JerzyS.1923:1990.‘‘OntheApplicationofProbabilityTheorytoAgri-
Cultural Experiments. Essay on Principles. Section 9’’ (with discussion).

StatisticalScience4:465–80.

Pearl,Judea.2000.Causality.Cambridge,England:CambridgeUniversityPress.

Robins, James M. 1989. ‘‘The Analysis of Randomized and Non-Randomized
AIDS Trials Using a New Approach to Causal Inference in Longitudinal
Studies.’’ Pp. 113–59 in Health Service Research Methodology: A Focus on
AIDS, edited by Lee Sechrest, Howard Freeman, and Albert Mulley.

Washington, DC: U.S. Public Health Service, National Center for Health
ServicesResearch.

Robins,JamesM.,andSanderGreenland.2000.‘‘Commenton‘CausalInference
without Counterfactuals,’ by A. Philip Dawid.’’ Journal of the American
StatisticalAssociation95:431–35.

Rosenbaum,PaulR.2002.ObservationalStudies.2ded.NewYork:Springer.

Rosenbaum, Paul R., and Donald B. Rubin. 1983. ‘‘The Central Role of the
Propensity Score in Observational Studies for Causal Effects.’’ Biometrika
70:41–55.

Rosenzweig, Mark R., and Kenneth I. Wolpin. 2000. ‘‘Natural ‘Natural
Experiments’inEconomics.’’JournalofEconomicLiterature38:827–874.

NonrandomizedStudies.’’JournalofEducationalPsychology66:688–701.

———. 1977. ‘‘Assignment to Treatment Groups on the Basis of a Covariate.’’
JournalofEducationalStatistics2:1–26.

———. 1978. ‘‘Bayesian Inference for Causal Effects: The Role of
Randomization.’’AnnalsofStatistics6:34–58.

———. 1980. ‘‘Comment on ‘Randomization Analysis of Experimental Data:
The Fisher Randomization Test,’ by D. Basu.’’ Journal of the American
StatisticalAssociation75:591–93.

Smith, Herbert L. 1997. ‘‘Matching with Multiple Controls to Estimate
Treatment Effects in Observational Studies.’’ Pp. 325–53 in Sociological
Methodology, vol. 27, edited by Adrian E. Raftery. Boston, MA: Blackwell
Publishing.

Sobel, Michael E. 1990. ‘‘Effect Analysis and Causation in Linear Structural
EquationModels.’’Psychometrika55:495–515.

———.1995.‘‘CausalInferenceintheSocialandBehavioralSciences.’’Pp.1–38
in Handbook of Statistical Modeling for the Social and Behavioral Sciences,
edited by G. Arminger, C. C. Clogg, and M. E. Sobel. New York: Plenum
Press.

———. 2001. ‘‘Spatial Concentration and Social Stratification. Does the
Clustering of Disadvantage ‘Beget’ Bad Outcomes?’’ Forthcoming in Poverty
Traps, edited by S. Bowles, S. N. Durlauf, and K. Hoff. New York: Russel
SageFoundation.

———.2003.‘‘WhatDoRandomizedStudiesofHousingMobilityDemonstrate:
Causal Inference in the Face of Interference.’’ Unpublished manuscript,
ColumbiaUniversity.

Strotz, Robert H., and Herman O. A. Wold. 1960. ‘‘Recursive vs. Nonrecursive
Systems:AnAttemptatSynthesis(Part1).’’Econometrica28:417–27.

Vella,Francis.1998.‘‘EstimatingModelswithSampleSelectionBias:ASurvey.’’
JournalofHumanResources33:127–169.

Vytlacil,Edward.2002.‘‘Independence,Monotonicity,andLatentIndexModels:
AnEquivalenceResult.’’Econometrica70:331–41.

Whitehead, Alfred N. [1911] 1958. An Introduction to Mathematics. New York:
OxfordUniversityPress.

