---
title: 2025 Seoul Workshop on Philosophy of Machine Learning
---

**DAY 1 (Feb 25)**

| **09:30-10:00** | **Registration** |  | 
|---|---|---|
| **10:00-10:10** | **Opening** | **Welcoming Address** Sungkyu Jung (Director of IDIS, Seoul National University)**Opening Remarks** <br> Hyundeuk Cheon (Seoul National University)  | 
| **10:10-11:00** | **[Invited Talk 1](https://25swpml.wordpress.com/program/#InvitedTalk1)** | **Will the Advancement of AI Let Us Redefine Science?**<br> Insok Ko (Inha University) | 
| **11:00-11:10** | **Break** |  | 
| **11:10-12:00** | **[Invited Talk 2](https://25swpml.wordpress.com/program/#InvitedTalk2)** | **Predictively-valid “Alien” Features, or Artifacts? Coping with Inscrutable Scientific Progress**<br> Cameron Buckner (University of Florida) | 
| **12:00-13:30** | **Lunch** |  | 
| **13:30-14:50** | **[Contributed Talks 1](https://25swpml.wordpress.com/program/#CT1)** | **Beyond Tools: How Working with Coding AI Reshapes Cognition, Agency, and Subjectivity** Yubeen Kwon (Seoul National University)**Distributed and Probabilistic Model of Representation**<br> Lee-Sun Choi (Ewha Womans University) | 
| **14:50-15:10** | **Break** |  | 
| **15:10-16:00** | **[Invited Talk 3](https://25swpml.wordpress.com/program/#InvitedTalk3)** | **Isolationism for Holists** Andre Curtis-Trudel (University of Cincinnati)  Emily Sullivan (Utrecht University) | 
| **16:00-16:10** | **Break** |  | 
| **16:10-18:10** | **[Contributed Talks 2](https://25swpml.wordpress.com/program/#CT2)** | **Artificial Possibilities** Bojana Grujičić (Max Planck School of Cognition) **Augmented Intellect: AI as Extended Mind in Scientific Understanding** Injin Woo (Sungkyunkwan University)**Autonomous Weapon Systems and Just War Theory: A Critical Reevaluation of *Jus in Bello*Principle**<br> Sangsu Kim (Korea Military Academy) | 
**DAY 2 (Feb 26)**

| **09:30-09:40** | **Registration** |  | 
|---|---|---|
| **09:40-10:30** | **[Invited Talk 4](https://25swpml.wordpress.com/program/#InvitedTalk4)** | **AI and the Logic of Scientific Discovery**<br> Yeongseo Yeo (Dongduk Women’s University) | 
| **10:30-10:40** | **Break** |  | 
| **10:40-12:00** | **[Contributed Talks 3](https://25swpml.wordpress.com/program/#CT3)** | **Prediction, Projection, and Performativity** Konstantin Genin (University of Tübingen) **Predicting Black Swan Events: The Final Frontier of Machine Learning?**<br> Wonki Her (Seoul National University) | 
| **12:00-13:30** | **Lunch** |  | 
| **13:30-14:20** | **[Invited Talk 5](https://25swpml.wordpress.com/program/#InvitedTalk5)** | **On Finding What You’re (not) Looking for: Prospects and Challenges for AI-driven Discovery**<br> Andre Curtis-Trudel (University of Cincinnati) | 
| **14:20-14:30** | **Break** |  | 
| **14:30-15:20** | **[Invited Talk 6](https://25swpml.wordpress.com/program/#InvitedTalk6)** | **The Moral Importance of Explainable AI**<br> Kate Vredenburgh (London School of Economics and Political Science) | 
| **15:20-15:40** | **Break** |  | 
| **15:40-16:30** | **[Invited Talk 7](https://25swpml.wordpress.com/program/#InvitedTalk7)** | **Foundation Models in Healthcare Require Rethinking Reliability**<br> Thomas Grote (University of Tübingen) | 
| **16:30-16:40** | **Break** |  | 
| **16:40-18:00** | **[Contributed Talks 4](https://25swpml.wordpress.com/program/#CT4)** | **Beyond Opacity: Why Implementation Details Matter in Understanding ML Models** Hyung Suk Lee (Seoul National University)**AI as a Pathway to Scientific Knowledge?** Nikolaj Jang Lee Linding Pedersen (Yonsei University)  Jens Christian Bjerring (Aarhus University) | 
| **18:00-18:10** | **Closing** |  | 
**ABSTRACTS**

**Invited Talk 1**

**Will the Advancement of AI Let Us Redefine Science?**

Insok Ko (Inha University)

According to T. S. Kuhn, science is a collective activity of solving still unsolved puzzles recognized in a specialized domain of research, and this activity is guided by one or more exemplary instances of concrete problem solving. Today, we have more and more cases of scientific research in which AIs are utilized as essential tools. Though the AI agents do not have subjective motivation to solve the puzzle, they will emerge as powerful actors that solve some of the puzzles in a Kuhnian way, or at least help (human-) scientists do it. This would imply that the concepts of science, scientists, and scientific research will change. Will the advancement of AI induce us to redefine science? I will discuss this question from a couple of perspectives.

**Invited Talk 2**

**Predictively-valid “Alien” Features, or Artifacts? Coping with Inscrutable Scientific Progress**

Cameron Buckner (University of Florida)

Systems like AlphaFold raise the prospect of predictive AI systems that can blow past previous upper bounds on the performance of hand-designed analytical models in many areas of scientific analysis. It is difficult to disagree with the results of these systems, which can achieve predictive accuracy on problems that were thought to be too complex or chaotic for human scientific theory to solve. However, these models may base their predictions on features that are in some sense beyond the cognitive grasp of humans–“alien” properties that may have predictive utility but which are not natural or cognitively accessible to us. In this talk I will analyze these properties by beginning with a discussion of adversarial attacks, and discuss methods for coping with this epistemic situation in a scientific regime which increasingly relies on complex deep learning models for data analysis.

**Invited Talk 3**

**Isolationism for Holists**

Andre Curtis-Trudel (University of Cincinnati)

Emily Sullivan (Utrecht University)

Models often idealize their target phenomenon; but when are idealizations successful and how might we evaluate idealizations? Holists argue that justifying and evaluating models and their idealizations must proceed holistically and cannot be a function of aggregating or assessing individual parts of models. The holist argument comes from different pressure points. For some highly complex models it seems practically unfeasible to isolate where model failures occur due to opacity and complexity in interactions between different parts of the model. In other cases, it seems that isolationism suffers from a more fundamental problem: idealizations cannot be removed or isolated without radically changing the character of the model itself. In this talk, we look at the example of AlphaFold, a highly complex model in machine learning. AlphaFold is a paradigmatic example of the kind of highly complex model well suited for the holist project. However, we argue that decomposition and isolationism are necessary (and indeed possible) for evaluating the AlphaFold’s idealizations and the way they contribute to its success. As a result, we argue that holists should endorse the isolationist project.

**Invited Talk 4**

**AI and the Logic of Scientific Discovery**

Yeongseo Yeo (Dongduk Women’s University)

Is deep learning AI reliable? AlphaFold2, a deep learning AI renowned for its accurate protein structure predictions, won the 2024 Nobel Prize in Chemistry. Its empirical success is undeniably impressive. However, many philosophers remain concerned about its epistemic opacity. Like other deep learning models, AlphaFold2 operates as a “black box,” meaning that its internal decision-making processes are not entirely interpretable. This lack of transparency raises fundamental questions: When should we rely on deep learning AI, and when not? What conditions must deep learning AI meet to be considered reliable? Can scientific knowledge be derived solely from deep learning AI? Philosophers have extensively debated these concerns, and with the groundbreaking success of AlphaFold2 and AlphaFold3, it is time to raise a new question: Can their empirical achievements challenge the skeptical or pessimistic views of philosophers on AlphaFold and deep learning AI?

Duede (2023) suggests that deep learning AI may be valuable in the context of discovery, where it generates plausible hypotheses, but not in the context of justification, where scientific claims require rigorous testing and confirmation. But is this old distinction the only way to defend deep learning AI? Should we conclude that an epistemically opaque system like AlphaFold2 is ultimately unreliable?

Ortmann (2025) challenges this skepticism by distinguishing between why-reliability and whether-reliability. He argues that even if we do not fully understand why AlphaFold2 is reliable, we can still determine whether it is reliable or not. The whether-reliability of alphaFold2 has already been demonstrated through its empirical success. Furthermore, its widespread adoption in contemporary scientific research provides additional support for its whether-reliability. After all, an unreliable tool would not be so extensively used in scientific practice.

Yet, many skeptical philosophers would remain unconvinced. For example, Duede (2022) insists that brute inductive considerations are never enough, that is, transparency is essential for true reliability. Similarly, Mitchell (2020) argues that empirical success must be accompanied by epistemic warrant, since reliability arises from the combination of theory’s epistemic warrant and the stability of supporting evidence.

This presentation explores, in response to Duede and Mitchell, whether the Logic of Discovery, understood as a set of problem-solving heuristics, can provide the necessary epistemic warrant for AlphaFold and deep learning AI. The idea is that the methods and principles underlying AlphaFold’s development may serve as a foundation for establishing its why-reliability. By articulating the heuristics that guided the creation of AlphaFold, we may be able to offer sufficient epistemic warrant to address concerns about the transparency required for AlphaFold and deep learning AI.

**Invited Talk 5**

**On Finding What You’re (not) Looking for: Prospects and Challenges for AI-driven Discovery**

Andre Curtis-Trudel (University of Cincinnati)

Recent high-profile scientific achievements by machine learning (ML) and especially deep learning (DL) systems have reinvigorated interest in ML for automated scientific discovery (e.g., Wang et al. 2023). Much of this work is motivated by the thought that DL methods might facilitate the efficient discovery of phenomena, hypotheses, or even models or theories more efficiently than traditional, theory-driven approaches to discovery. This talk considers some of the more specific obstacles to automated, DL-driven discovery in frontier science, focusing on gravitational-wave astrophysics (GWA) as a representative case study. In the first part of the talk, we argue that despite these efforts prospects for DL-driven discovery in GWA remain uncertain. In the second part, we advocate a shift in focus towards the ways DL can be used to augment or enhance existing discovery methods, and the epistemic virtues and vices associated with these uses. We argue that the primary epistemic virtue of many such uses is to decrease opportunity costs associated with investigating puzzling or anomalous signals, and that the right framework for evaluating these uses comes from philosophical work on pursuitworthiness.

**Invited Talk 6**

**The Moral Importance of Explainable AI**

Kate Vredenburgh (London School of Economics and Political Science)

Understanding AI systems is important not just epistemically, but also morally. In this talk, I will do three things. First, I will argue an account of the moral values that are important for two different types of AI systems, what we might call predictive machines versus agents. I argue that enabling individuals to engage in informed self-advocacy is important for predictive machines, whereas values like trust and interpersonal respect are important for agents. The type of AI system thus has implications for what kind of explanations we should aim at, and corresponding scientific paradigms in XAI. Second, for predictive machines, I will use the example of credit scoring to argue that we face a fundamental tension between explaining how to be someone who has some property P and how to be judged to be someone who has some property P. Because of static modeling choices, and uncertainty, the two come apart. While the latter is attractive from a mathematical perspective, it requires background institutions that lessen the cost of mistakes to individuals. The former, however, may not suitably enable informed self-advocacy. Third, I will argue that interpersonal justification is important for agents, even artificial agents, but that providing justifications requires a responsiveness to reasons and social norms that artificial agents lack.

**Invited Talk 7**

**Foundation Models in Healthcare Require Rethinking Reliability**

Thomas Grote (University of Tübingen)

A new class of AI models, called foundation models, has entered healthcare. Foundation models violate several basic principles of the standard machine learning paradigm for assessing reliability – they are tested with data they might have seen during training, trained with data from different and oftentimes opaque sources, and generate outputs whose adequacy for purpose is difficult to assess. These violations make it necessary to rethink what guarantees we require to establish warranted trust into foundation models.

**Contributed Talks 1**

**Beyond Tools: How Working with Coding AI Reshapes Cognition, Agency, and Subjectivity**

Yubeen Kwon (Seoul National University)

What does it mean to work alongside AI? What changes are required to collaborate effectively? How does collaborating with AI reshape our understanding of cognition, agency, and subjectivity?

AI is distinguished from previous technologies in its ability to perform functions traditionally associated with human cognition, such as learning, problem-solving, and autonomous decision-making. Since the release of OpenAI’s ChatGPT, workplaces across various sectors have explored ways to integrate generative AI, sparking critical questions about its effective use in workflows and the management of human-AI teams.

In the software development community, coding AI tools such as GitHub Copilot have rapidly gained traction, becoming integral to developers’ practices. This presentation draws on in-depth interviews with 30 developers who routinely use coding AI—including GitHub Copilot, Code Whisperer, Claude, and ChatGPT—as well as observational research on their workflows. The developers formed relationships with AI that go beyond just using it as a tool – they treat it as an assistant, partner, and sometimes even as an extension of themselves. Notably, developers actively “teach” AI their intentions and goals, gradually shaping them into their ‘digital doppelgängers’ through iterative interactions and feedback. This shows how developers aren’t just delegating tasks to AI – they’re sharing their thinking processes with it.

Using cognitive and human-machine interface frameworks, this presentation explores the dynamic unfolding of agency in human-AI collaboration, emphasizing the roles of both human and non-human agencies. Frameworks such as distributed cognition and cognitive assemblage illuminate how cognitive processes are redistributed and reconfigured in these interactions. Developers increasingly view AI not merely as a tool but as an extension of their own cognitive processes, resulting in what can be termed a cognitive assemblage. Through analysis of the interactions between human developer and coding AI, this presentation demonstrates the necessity of an expanded concept of subjectivity that reflects the complex entanglements of cognition and agency in everyday interactions with AI.

**Distributed and Probabilistic Model of Representation**

Lee-Sun Choi (Ewha Womans University)

How is the representation of the world constructed? Roughly speaking, a representation system receives data from external sources, encodes it through its input system, and combines this encoded information in the internal system to deliver and maintain it in working memory. Our world representation is not solely composed of information about individual entities, such as Garfield, Tom, or Cheshire, but also includes representations of categories, such as cats. These categories are called concepts. We represent the world using concepts.

Research on representational systems in artificial intelligence can be broadly divided into two processes. The first is the process of concept application, which explains how newly provided information is categorized within an existing conceptual framework. The second is the process of concept formation, which describes how new concepts are created based on data. This study focuses on the latter process, discussing how world representation is determined through concept formation.

This study proposes a new representational theory that integrates two major theories in artificial intelligence and cognitive science: distributed representation and probabilistic representation. Distributed representation, a core theory in deep learning, particularly in natural language processing and computer vision, places data in a feature space composed of multiple dimensions. For example, an image of a cat is represented by extracting low-level features such as contours, edges, colors, and textures, which are then combined into more complex features like “having ears” or “having a tail.” This feature space can be seen as a high-dimensional coordinate system, where each feature serves as an axis, and the input data is represented as a single point, or vector, in this space. A point can be understood as a combination of various features.

Probabilistic representation models, on the other hand, interpret the process of concept formation as analyzing clusters of data in the feature space using specific likelihood functions. Dense data clusters, especially those isolated from other clusters, are more likely to be individuated as single concepts. In contrast, sparse clusters or patterns with multiple clusters are more likely to be interpreted as multiple concepts. This model opens the possibility for various observers to construct equivalent world representations based on the same data, assuming that no privileged world representation, such as a “God’s view,” exists. This aligns with the characteristics of human representation and also sets the conditions for the emergence of new concepts.

However, if different world representation models exist, they must be translatable into one another; otherwise, each model risks being solipsistically isolated. This study proposes a method to evaluate the agreement of concepts in different feature spaces using quantitative metrics based on Kullback-Leibler divergence, assessing the consistency and differences between concepts.

Nevertheless, integrating distributed and probabilistic representations entails several limitations. For instance, the composition of the feature space—what features and how many of them are included—can lead to entirely different clusters for the same external input data. This discrepancy makes translating concept-based clusters across models challenging and highlights the constraints of assuming that all world representation models share the same dimensional structure of the feature space. This study explores the implications of these limitations for designing representation models and investigates the possibilities and boundaries of an integrated representational theory.

**Contributed Talks 2**

**Artificial Possibilities**

Bojana Grujičić (Max Planck School of Cognition)

Science often deals with issues pertaining to possibilities, contingencies and necessities, by engaging in thought experiments and modeling. This talk discusses how much modal scientific understanding can be obtained by the use of deep neural networks in cognitive science. One epistemically useful feature of neural networks is their runnability – they can be trained to perform a cognitive task and can run when given novel stimuli, demonstrating possibilities of cognitive phenomena based on sets of inductive biases. I focus on the problem of justification of neural network-based inferences about possibilities and outline a plausible justificatory strategy. I consider a number of reasons for taking neural network-demonstrated possibilities to be technological, rather than biological possibilities. Despite this, I argue that they add to our modal scientific understanding of cognitive phenomena.

**Augmented Intellect: AI as Extended Mind in Scientific Understanding**

Injin Woo (Sungkyunkwan University)

In this paper, I argue that Artificial Intelligence (AI) advances scientific understanding through the mechanism of (socially) extended cognition. Many researchers speculate that AI, upon reaching AGI, may satisfy certain conditions for agency, such as intentionality and autonomy. By examining the conceptual framework of extended cognition and integrating perspectives from Pritchard and Khalifa’s epistemologies, I demonstrate how AI transcends traditional tool-based cognition to become an active participant in scientific understanding.

AI can be considered extended cognition, as defined by Chalmers and Clark (1998). Extended cognition involves integrating human cognitive processes with external environments or tools. Three essential conditions must be met: reliable coupling, direct availability, and automatic endorsement. AI is deeply integrated into human life, operating consistently and dependably (e.g., AI assistants). It responds instantly to questions or commands, supporting human cognitive demands in real time. AI systems earn user trust through repeated use and positive experiences. Just as smartphones have demonstrated their potential to function as extended cognition—despite initial technical limitations—AI also holds the potential to constitute extended cognition (Helliwell, 2019).

Building upon the concept of extended cognition, we now turn to the idea of socially extended cognition. Pritchard (2022) explains scientific knowledge by emphasizing how collaboration contributes to understanding. Socially extended cognition occurs when the information-processing activities of others are integrated into an individual’s cognitive process, leading to knowledge beyond individual capability. If AI possesses agency, it can function as part of our socially extended cognition. AI supports and complements cognitive processes through collaboration with humans, unlike human collaboration, which may lack direct availability. AI minimizes or eliminates these limitations, reinforcing its role as a form of socially extended cognition. Pritchard limited his discussion to scientific knowledge, but I aim to integrate it with research on scientific understanding.

To grasp how this socially extended cognition enhances not just scientific knowledge, but also scientific understanding, we turn to Khalifa’s EKS model. Khalifa emphasizes that knowledge, particularly explanatory knowledge acquired through scientific explanatory evaluation (SEEing), is at the heart of understanding. SEEing in the EKS Model is a process of considering and comparing a range of plausible potential explanations, then selecting the most compelling one based on evidence and other explanatory considerations. This process of scientific inquiry goes beyond mere belief, leading to well-justified, reliable understanding that minimizes accidental or coincidental elements. Consequently, AI plays a pivotal role in enhancing SEEing, whether as a tool or as a collaborative partner.

Building on this discussion, a critical review of scientists’ research (Krenn et al., 2022) on how AI contributes to scientific understanding highlights three key dimensions: (1) computational microscope, (2) resource of inspiration, and (3) agent of understanding. As a data processing and evaluation tool, AI enhances researchers’ ability to compare potential explanations and identify the most compelling ones. As a collaborator, AI generates hypotheses, refines frameworks, and enhances human cognition, thereby strengthening scientific evaluation. Through these dual roles, AI transforms the processes of SEEing, ultimately contributing to enhanced scientific understanding.

**Autonomous Weapon Systems and Just War Theory: A Critical Reevaluation of*****Jus in Bello*****Principle**

Sangsu Kim (Korea Military Academy)

The emergence of Autonomous Weapon Systems (AWS), driven by the revolutionary advancements in artificial intelligence (AI), has fundamentally transformed modern warfare, calling for a critical reevaluation of the jus in bello principles within the framework of Just War Theory (JWT). AWS, as a product of AI innovation, operate independently, making critical decisions without human intervention, thereby enhancing operational efficiency. However, their autonomy raises serious ethical concerns about their ability to adhere to the core principles of discrimination and proportionality under JWT. These concerns are particularly significant given that AWS are weapon systems involving decisions that directly impact human lives, underscoring the urgent need for robust ethical frameworks to govern their use.

Jus in bello is a set of ethical principles that govern the conduct of war, requiring all moral agents involved in warfare to uphold its standards. Widely accepted as a foundational moral guideline in international contexts, these principles serve as the minimum ethical framework for ensuring the dignity and rights of individuals, even in the unavoidable circumstances of war. In particular, discrimination and proportionality are key to maintaining the moral legitimacy of warfare by protecting innocent civilians and ensuring that the use of force remains strictly aligned with legitimate military objectives.

The principle of discrimination requires a clear distinction between combatants and non-combatants, safeguarding civilians and other protected entities from unnecessary harm. This principle assumes that decision-makers can assess complex battlefield scenarios and make judgments consistent with international humanitarian law. Nonetheless, AWS, relying on preprogrammed algorithms and datasets, face significant challenges in identifying non-combatants accurately in dynamic and unpredictable environments. When incomplete or biased datasets influence AWS decision-making, there is a heightened risk of serious violations of the principle of discrimination.

The principle of proportionality demands that the harm caused by military actions be balanced against the anticipated military advantage. This principle requires nuanced assessments that go beyond mere calculations, incorporating moral and contextual factors. Yet, AWS operate based on predefined parameters, potentially failing to account for the complexity of real-world situations. The possibility of unforeseen outcomes further calls into question whether AWS can consistently uphold the proportionality principle in practice.

This presentation critically examines whether AWS can effectively adhere to the principles of discrimination and proportionality. It evaluates the technical and ethical capabilities of AWS to distinguish between combatants and non-combatants, avoid unlawful targets, and make decisions that align with human moral standards. Furthermore, it explores whether the use of lethal force by AWS can be justified under the principle of proportionality and discusses the extent to which human oversight is necessary to ensure compliance with these ethical principles.

The presentation is structured around three key discussions. First, it analyzes the technical characteristics and operational roles of AWS to assess whether they align with the established principles of JWT. Second, it reevaluates the ethical and practical interpretations of discrimination and proportionality to determine their applicability to AWS. Third, it proposes alternative approaches to reconfiguring jus in bello principles to reflect the ethical challenges posed by advanced technologies like AWS in modern warfare.

Ultimately, this presentation highlights the ethical urgency of scrutinizing AWS, given their origins in AI innovation and their life-and-death implications. It seeks to provide a framework for assessing AWS within the ethical boundaries of jus in bello, aiming to balance technological innovation with moral accountability. This approach contributes to redefining the ethical standards of modern warfare while ensuring justice and humanity remain central in the context of evolving conflicts.

**Contributed Talks 3**

**Prediction, Projection, and Performativity**

Konstantin Genin (University of Tübingen)

It is widely recognized that in social contexts predictions are not mere causally inert prognostications about future outcomes, but have a causal impact on the outcomes that they intend to predict. This is especially the case when predictions directly inform policy decisions. When predictions can be self-fulfilling, or self-undermining, pragmatic and epistemic considerations are entangled, and the notion of accuracy becomes ambiguous. Moreover, the notion of inductive risk must be revisited: values do not only come into play because the consequences of different kinds of error must be weighed, but also because we worry that predictions might be right for the wrong reasons. Many proposals have been made on how to deal with performativity, including “endogenizing” performative effects, or making predictions that steer outcomes in socially desirable directions. We argue that these approaches are misguided. Many undesirable performative effects arise because the outputs of machine learning models are ambiguous between predictions that predict outcomes by implicitly guessing which policy will be adopted, and counterfactual projections that predict outcomes were some policy option selected. Once we make the right distinction between prediction and projections, pragmatic and epistemic issues can be cleanly distinguished. We argue that only by aiming at accurate projections can machine learning models be not only decision-supportive but discourse-supportive: furnishing the relevant projections to allow discussants with a variety of goals and values to engage in discourse about which of a set of policy options represents the best reflective compromise.

**Predicting Black Swan Events: The Final Frontier of Machine Learning?**

Wonki Her (Seoul National University)

A Black Swan event is an event with the following three principal characteristics: (i) It is an outlier. (ii) It carries an extreme impact. (iii) After the event, we create an explanation (or a reasonable story) that makes it appear less random and more predictable than it was. World War I, the dissolution of the Soviet Union, the 9/11 attacks, and the death of the Inductivist Turkey are paradigmatic examples. Despite their outlier status, many people want to predict Black Swan events. This is because, if we can predict these Black Swan events, we can defend our lives from disastrous situations. Recently, some machine learning researchers have been trying to build Black Swan prediction machines. However, are these meaningful trials?

In this presentation, I will present a skeptical argument against building Black Swan prediction machines. For successful machine predictions, the following conditions need to be satisfied: (i) Large sets of high-quality training data, (ii) A well-defined prediction task, (iii) A well-defined target benchmark or specific standard to assess the prediction model, and (iv) Stable background conditions in the target domain.

As we know, in recent years, machine learning models have demonstrated incredible success (e.g., in Go, image recognition, and protein structure prediction). In these cases, the design and training of machine learning models meet the conditions above well. However, unlike games or natural science research, predicting rare social events is difficult because those conditions are hard to satisfy.

First, Black Swan events are very rare, making it difficult to prepare a sufficiently large and high-quality dataset for discovering patterns. Second, defining the prediction task itself is challenging. For instance, should we define it as an event in which 5% of the global population dies within five years? Or should it be defined as an event causing a loss of 1 trillion dollars? Similarly, it is difficult to establish criteria for evaluating prediction models. Most importantly, because our social environment is constantly changing, it is unlikely that a successful Black Swan prediction model will remain effective in the future.

Beyond environmental changes, we also adjust our behavior based on predictions, which introduces the so-called “Preparedness Paradox.” If we predict a Black Swan event, we will take actions to prevent its occurrence. If these actions are successful, the Black Swan event will not happen. The problem is that in this case, we cannot determine whether the prediction model was useful or not because we do not have something like a twin Earth to test our predictions or treatments.

In conclusion, building a Black Swan prediction model is not only extremely difficult but also challenging to evaluate, even if we succeed in creating one.

**Contributed Talks 4**

**Beyond Opacity: Why Implementation Details Matter in Understanding ML Models**

Hyung Suk Lee (Seoul National University)

Scientists rely on hypotheses, theories, and models to grasp and justify phenomena. In contrast, Machine Learning (ML) models, introduced to compensate for the limitations of human cognitive capacity, do not rely on theories, laws, mechanisms, or scientific models. Instead, ML models seek a data model that represents the correct correspondence between the input and output data provided for training. Consequently, the data-driven models produced by ML are largely opaque and incomprehensible to humans.

In this situation, there is concern that the scientific values of explanation and understanding will be undermined if scientists increasingly adopt opaque and less understandable ML models for prediction or inference. As the use of ML for scientific purposes is steadily growing, the inability to understand and explain the predictive results produced by ML raises the question of whether the goal of science is shifting from explanation to mere prediction.

In response, Sullivan(2022) argues that we are unnecessarily worried due to an improper diagnosis of the phenomenon in which our scientific understanding is limited when using ML models. The opacity of ML models does not harm our understanding; the real problem, she claims, is the lack of scientific evidence linking ML models to phenomena. She argues that to promote explanation and understanding of the real world, we must secure evidence connecting models and phenomena, and that knowing more about the internal implementation state of a model does not increase our understanding of the world.

I believe that Sullivan’s claim that implementation opacity does not hinder understanding of ML models is incorrect. She has merely turned her head away from a harsh reality. Without resolving implementation opacity, we cannot understand the predictions of ML models.

First, I will analyze the three examples provided by Sullivan (Schelling’s segregation model, melanoma detection model, and sexual orientation prediction model) to clarify what she means by “link uncertainty.” This analysis will demonstrate that the connection between model and phenomenon, as Sullivan describes it, is formed through two stages: first, replacing the ML model with a human-cognizable surrogate model, and second, connecting this surrogate model to a traditional scientific model that represents the phenomenon. Next, I will show that if we fail to grasp the implementation process of the ML model, even if the first stage of connection is achieved by replacing the ML model with a human-cognizable surrogate model using sufficiently developed xAI techniques, the success of the second stage can be highly precarious. Consequently, I argue that Sullivan’s claim that we can gain scientific understanding through ML models even without resolving implementation opacity is unacceptable, because without resolving implementation opacity, we cannot resolve link uncertainty.

**AI as a Pathway to Scientific Knowledge?**

Nikolaj Jang Lee Linding Pedersen (Yonsei University)

Jens Christian Bjerring (Aarhus University)

Artificial Intelligence is increasingly positioned as a transformative tool in the pursuit of scientific knowledge, reshaping how we collect, analyze, and interpret data. But under what conditions can an AI system produce scientific knowledge?

Reliability is required for knowledge (or so we shall assume here). A true belief qualifies as knowledge only if it is formed via a reliable belief-forming method or process (i.e. one that generates mostly true beliefs). Thus, true AI-based scientific beliefs qualify as knowledge only if they are formed via reliable AI methods or belief-forming processes (i.e. methods or processes that take AI outcomes as input and deliver beliefs as outputs). We argue that, while reliability may be a necessary condition for AI-based knowledge, it—together with truth—is not sufficient. Reliability must be supplemented by a condition which ensures that the relevant AI belief-forming method or process does not take highly inaccurate inputs.

We home in on this condition through considerations on the distinctive nature of the types of errors that AI systems may make.  AI systems can sometimes make “crazy” or “astounding” errors—e.g., classifying a bird as a car. These kinds of errors seem qualitatively different from those that humans would typically make. They do not merely reflect minor inaccuracies but instead reveal extreme failures seemingly disconnected from the data being processed.

It is epistemically risky to form beliefs via methods or processes that take such mistaken AI-generated outcomes as inputs, as it undermines the prospects of knowledge. This point is articulated and argued within a framework featuring two levels of epistemic evaluation: the epistemic value of belief outputs of methods or processes and the accuracy of their inputs. Epistemic value  is trichotomous. There are three epistemic values: truth, falsity, and knowledge. Accuracy takes values in the unit interval, i.e. real numbers in [0; 1].

The condition that belief-forming methods and processes not take highly inaccurate inputs is a maximin-style condition. The minimum accuracy of inputs must not be too low. Failing this condition means forming beliefs via a method or process that sometimes takes highly inaccurate inputs. This is epistemically risky as it is incompatible with knowledge. Hence, highly inaccurate inputs undermine the prospects of knowledge.

We offer the following diagnosis of the connection between high inaccuracy and knowledge: for a belief-forming method or process that takes outputs from source S as input, the competence of S is undermined if some of S’s outputs are wildly inaccurate. However, a source’s failure to be competent means that it cannot serve as a basis for knowledge—even if forming beliefs on the basis of its deliverances leads to the formation of mostly true beliefs. Thus, AI systems that sometimes deliver highly inaccurate outputs cannot serve as the basis of knowledge, even if most beliefs formed on the basis of their outputs are true. This is because such AI systems fail to exhibit competence.

[Program](https://25swpml.wordpress.com/program/)